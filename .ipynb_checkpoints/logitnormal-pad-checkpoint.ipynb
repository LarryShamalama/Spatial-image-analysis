{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "\n",
    "from lib.car_model import CAR2\n",
    "from lib.utils import pad, new_name, create_matrices, get_digit_indices\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from theano import scan\n",
    "import theano.tensor as tt\n",
    "\n",
    "from pymc3.distributions import continuous\n",
    "from pymc3.distributions import distribution\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train/255\n",
    "x_test  = x_test/255\n",
    "\n",
    "def subset(label, data=x_train, labels=y_train):\n",
    "    '''\n",
    "    e.g. subset(3) -> gets all pictures of digit 3\n",
    "    '''\n",
    "    assert label >= 0 and label <= 9\n",
    "    \n",
    "    return data[np.argwhere(labels == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, figsize=(16, 7)):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 314,
       "width": 300
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy = x_train[12]\n",
    "plot(dummy, figsize=(10, 5))\n",
    "_ = plt.title('Label: ' + str(y_train[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dummy).to_csv('images/handwritten_digit.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.concatenate(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from https://docs.pymc.io/notebooks/PyMC3_tips_and_heuristic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "position_matrix = np.linspace(0, 28*28 - 1, num=28*28).astype(np.int64).reshape(28, 28)\n",
    "count = 0\n",
    "\n",
    "for i, row in enumerate(position_matrix):\n",
    "    for j, col in enumerate(position_matrix[i]):\n",
    "        assert position_matrix[i][j] == col\n",
    "        \n",
    "        temp = []\n",
    "\n",
    "        # change these loops if we do not want to\n",
    "        # include diagonal elements in adj matrix\n",
    "        for delta_i in [-1, 0, 1]:\n",
    "            for delta_j in [-1, 0, 1]:\n",
    "                if ((i + delta_i) // 28 == 0) and ((j + delta_j) // 28 == 0):    \n",
    "                    temp.append(position_matrix[i + delta_i][j + delta_j])\n",
    "        \n",
    "\n",
    "        temp.remove(col)\n",
    "        temp.sort()\n",
    "        adj.append(temp)\n",
    "        \n",
    "weights = [list(np.ones_like(adj_elems).astype(np.int64)) for adj_elems in adj]\n",
    "\n",
    "# below is taken from the pymc3 CAR tutorial website\n",
    "maxwz = max([sum(w) for w in weights])\n",
    "N = len(weights)\n",
    "wmat2 = np.zeros((N, N))\n",
    "amat2 = np.zeros((N, N), dtype='int32')\n",
    "for i, a in enumerate(adj):\n",
    "    amat2[i, a] = 1\n",
    "    wmat2[i, a] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 28, 29],\n",
       " [0, 2, 28, 29, 30],\n",
       " [1, 3, 29, 30, 31],\n",
       " [2, 4, 30, 31, 32],\n",
       " [3, 5, 31, 32, 33],\n",
       " [4, 6, 32, 33, 34],\n",
       " [5, 7, 33, 34, 35],\n",
       " [6, 8, 34, 35, 36],\n",
       " [7, 9, 35, 36, 37],\n",
       " [8, 10, 36, 37, 38],\n",
       " [9, 11, 37, 38, 39],\n",
       " [10, 12, 38, 39, 40],\n",
       " [11, 13, 39, 40, 41],\n",
       " [12, 14, 40, 41, 42],\n",
       " [13, 15, 41, 42, 43],\n",
       " [14, 16, 42, 43, 44],\n",
       " [15, 17, 43, 44, 45],\n",
       " [16, 18, 44, 45, 46],\n",
       " [17, 19, 45, 46, 47],\n",
       " [18, 20, 46, 47, 48],\n",
       " [19, 21, 47, 48, 49],\n",
       " [20, 22, 48, 49, 50],\n",
       " [21, 23, 49, 50, 51],\n",
       " [22, 24, 50, 51, 52],\n",
       " [23, 25, 51, 52, 53],\n",
       " [24, 26, 52, 53, 54],\n",
       " [25, 27, 53, 54, 55],\n",
       " [26, 54, 55],\n",
       " [0, 1, 29, 56, 57],\n",
       " [0, 1, 2, 28, 30, 56, 57, 58],\n",
       " [1, 2, 3, 29, 31, 57, 58, 59],\n",
       " [2, 3, 4, 30, 32, 58, 59, 60],\n",
       " [3, 4, 5, 31, 33, 59, 60, 61],\n",
       " [4, 5, 6, 32, 34, 60, 61, 62],\n",
       " [5, 6, 7, 33, 35, 61, 62, 63],\n",
       " [6, 7, 8, 34, 36, 62, 63, 64],\n",
       " [7, 8, 9, 35, 37, 63, 64, 65],\n",
       " [8, 9, 10, 36, 38, 64, 65, 66],\n",
       " [9, 10, 11, 37, 39, 65, 66, 67],\n",
       " [10, 11, 12, 38, 40, 66, 67, 68],\n",
       " [11, 12, 13, 39, 41, 67, 68, 69],\n",
       " [12, 13, 14, 40, 42, 68, 69, 70],\n",
       " [13, 14, 15, 41, 43, 69, 70, 71],\n",
       " [14, 15, 16, 42, 44, 70, 71, 72],\n",
       " [15, 16, 17, 43, 45, 71, 72, 73],\n",
       " [16, 17, 18, 44, 46, 72, 73, 74],\n",
       " [17, 18, 19, 45, 47, 73, 74, 75],\n",
       " [18, 19, 20, 46, 48, 74, 75, 76],\n",
       " [19, 20, 21, 47, 49, 75, 76, 77],\n",
       " [20, 21, 22, 48, 50, 76, 77, 78],\n",
       " [21, 22, 23, 49, 51, 77, 78, 79],\n",
       " [22, 23, 24, 50, 52, 78, 79, 80],\n",
       " [23, 24, 25, 51, 53, 79, 80, 81],\n",
       " [24, 25, 26, 52, 54, 80, 81, 82],\n",
       " [25, 26, 27, 53, 55, 81, 82, 83],\n",
       " [26, 27, 54, 82, 83],\n",
       " [28, 29, 57, 84, 85],\n",
       " [28, 29, 30, 56, 58, 84, 85, 86],\n",
       " [29, 30, 31, 57, 59, 85, 86, 87],\n",
       " [30, 31, 32, 58, 60, 86, 87, 88],\n",
       " [31, 32, 33, 59, 61, 87, 88, 89],\n",
       " [32, 33, 34, 60, 62, 88, 89, 90],\n",
       " [33, 34, 35, 61, 63, 89, 90, 91],\n",
       " [34, 35, 36, 62, 64, 90, 91, 92],\n",
       " [35, 36, 37, 63, 65, 91, 92, 93],\n",
       " [36, 37, 38, 64, 66, 92, 93, 94],\n",
       " [37, 38, 39, 65, 67, 93, 94, 95],\n",
       " [38, 39, 40, 66, 68, 94, 95, 96],\n",
       " [39, 40, 41, 67, 69, 95, 96, 97],\n",
       " [40, 41, 42, 68, 70, 96, 97, 98],\n",
       " [41, 42, 43, 69, 71, 97, 98, 99],\n",
       " [42, 43, 44, 70, 72, 98, 99, 100],\n",
       " [43, 44, 45, 71, 73, 99, 100, 101],\n",
       " [44, 45, 46, 72, 74, 100, 101, 102],\n",
       " [45, 46, 47, 73, 75, 101, 102, 103],\n",
       " [46, 47, 48, 74, 76, 102, 103, 104],\n",
       " [47, 48, 49, 75, 77, 103, 104, 105],\n",
       " [48, 49, 50, 76, 78, 104, 105, 106],\n",
       " [49, 50, 51, 77, 79, 105, 106, 107],\n",
       " [50, 51, 52, 78, 80, 106, 107, 108],\n",
       " [51, 52, 53, 79, 81, 107, 108, 109],\n",
       " [52, 53, 54, 80, 82, 108, 109, 110],\n",
       " [53, 54, 55, 81, 83, 109, 110, 111],\n",
       " [54, 55, 82, 110, 111],\n",
       " [56, 57, 85, 112, 113],\n",
       " [56, 57, 58, 84, 86, 112, 113, 114],\n",
       " [57, 58, 59, 85, 87, 113, 114, 115],\n",
       " [58, 59, 60, 86, 88, 114, 115, 116],\n",
       " [59, 60, 61, 87, 89, 115, 116, 117],\n",
       " [60, 61, 62, 88, 90, 116, 117, 118],\n",
       " [61, 62, 63, 89, 91, 117, 118, 119],\n",
       " [62, 63, 64, 90, 92, 118, 119, 120],\n",
       " [63, 64, 65, 91, 93, 119, 120, 121],\n",
       " [64, 65, 66, 92, 94, 120, 121, 122],\n",
       " [65, 66, 67, 93, 95, 121, 122, 123],\n",
       " [66, 67, 68, 94, 96, 122, 123, 124],\n",
       " [67, 68, 69, 95, 97, 123, 124, 125],\n",
       " [68, 69, 70, 96, 98, 124, 125, 126],\n",
       " [69, 70, 71, 97, 99, 125, 126, 127],\n",
       " [70, 71, 72, 98, 100, 126, 127, 128],\n",
       " [71, 72, 73, 99, 101, 127, 128, 129],\n",
       " [72, 73, 74, 100, 102, 128, 129, 130],\n",
       " [73, 74, 75, 101, 103, 129, 130, 131],\n",
       " [74, 75, 76, 102, 104, 130, 131, 132],\n",
       " [75, 76, 77, 103, 105, 131, 132, 133],\n",
       " [76, 77, 78, 104, 106, 132, 133, 134],\n",
       " [77, 78, 79, 105, 107, 133, 134, 135],\n",
       " [78, 79, 80, 106, 108, 134, 135, 136],\n",
       " [79, 80, 81, 107, 109, 135, 136, 137],\n",
       " [80, 81, 82, 108, 110, 136, 137, 138],\n",
       " [81, 82, 83, 109, 111, 137, 138, 139],\n",
       " [82, 83, 110, 138, 139],\n",
       " [84, 85, 113, 140, 141],\n",
       " [84, 85, 86, 112, 114, 140, 141, 142],\n",
       " [85, 86, 87, 113, 115, 141, 142, 143],\n",
       " [86, 87, 88, 114, 116, 142, 143, 144],\n",
       " [87, 88, 89, 115, 117, 143, 144, 145],\n",
       " [88, 89, 90, 116, 118, 144, 145, 146],\n",
       " [89, 90, 91, 117, 119, 145, 146, 147],\n",
       " [90, 91, 92, 118, 120, 146, 147, 148],\n",
       " [91, 92, 93, 119, 121, 147, 148, 149],\n",
       " [92, 93, 94, 120, 122, 148, 149, 150],\n",
       " [93, 94, 95, 121, 123, 149, 150, 151],\n",
       " [94, 95, 96, 122, 124, 150, 151, 152],\n",
       " [95, 96, 97, 123, 125, 151, 152, 153],\n",
       " [96, 97, 98, 124, 126, 152, 153, 154],\n",
       " [97, 98, 99, 125, 127, 153, 154, 155],\n",
       " [98, 99, 100, 126, 128, 154, 155, 156],\n",
       " [99, 100, 101, 127, 129, 155, 156, 157],\n",
       " [100, 101, 102, 128, 130, 156, 157, 158],\n",
       " [101, 102, 103, 129, 131, 157, 158, 159],\n",
       " [102, 103, 104, 130, 132, 158, 159, 160],\n",
       " [103, 104, 105, 131, 133, 159, 160, 161],\n",
       " [104, 105, 106, 132, 134, 160, 161, 162],\n",
       " [105, 106, 107, 133, 135, 161, 162, 163],\n",
       " [106, 107, 108, 134, 136, 162, 163, 164],\n",
       " [107, 108, 109, 135, 137, 163, 164, 165],\n",
       " [108, 109, 110, 136, 138, 164, 165, 166],\n",
       " [109, 110, 111, 137, 139, 165, 166, 167],\n",
       " [110, 111, 138, 166, 167],\n",
       " [112, 113, 141, 168, 169],\n",
       " [112, 113, 114, 140, 142, 168, 169, 170],\n",
       " [113, 114, 115, 141, 143, 169, 170, 171],\n",
       " [114, 115, 116, 142, 144, 170, 171, 172],\n",
       " [115, 116, 117, 143, 145, 171, 172, 173],\n",
       " [116, 117, 118, 144, 146, 172, 173, 174],\n",
       " [117, 118, 119, 145, 147, 173, 174, 175],\n",
       " [118, 119, 120, 146, 148, 174, 175, 176],\n",
       " [119, 120, 121, 147, 149, 175, 176, 177],\n",
       " [120, 121, 122, 148, 150, 176, 177, 178],\n",
       " [121, 122, 123, 149, 151, 177, 178, 179],\n",
       " [122, 123, 124, 150, 152, 178, 179, 180],\n",
       " [123, 124, 125, 151, 153, 179, 180, 181],\n",
       " [124, 125, 126, 152, 154, 180, 181, 182],\n",
       " [125, 126, 127, 153, 155, 181, 182, 183],\n",
       " [126, 127, 128, 154, 156, 182, 183, 184],\n",
       " [127, 128, 129, 155, 157, 183, 184, 185],\n",
       " [128, 129, 130, 156, 158, 184, 185, 186],\n",
       " [129, 130, 131, 157, 159, 185, 186, 187],\n",
       " [130, 131, 132, 158, 160, 186, 187, 188],\n",
       " [131, 132, 133, 159, 161, 187, 188, 189],\n",
       " [132, 133, 134, 160, 162, 188, 189, 190],\n",
       " [133, 134, 135, 161, 163, 189, 190, 191],\n",
       " [134, 135, 136, 162, 164, 190, 191, 192],\n",
       " [135, 136, 137, 163, 165, 191, 192, 193],\n",
       " [136, 137, 138, 164, 166, 192, 193, 194],\n",
       " [137, 138, 139, 165, 167, 193, 194, 195],\n",
       " [138, 139, 166, 194, 195],\n",
       " [140, 141, 169, 196, 197],\n",
       " [140, 141, 142, 168, 170, 196, 197, 198],\n",
       " [141, 142, 143, 169, 171, 197, 198, 199],\n",
       " [142, 143, 144, 170, 172, 198, 199, 200],\n",
       " [143, 144, 145, 171, 173, 199, 200, 201],\n",
       " [144, 145, 146, 172, 174, 200, 201, 202],\n",
       " [145, 146, 147, 173, 175, 201, 202, 203],\n",
       " [146, 147, 148, 174, 176, 202, 203, 204],\n",
       " [147, 148, 149, 175, 177, 203, 204, 205],\n",
       " [148, 149, 150, 176, 178, 204, 205, 206],\n",
       " [149, 150, 151, 177, 179, 205, 206, 207],\n",
       " [150, 151, 152, 178, 180, 206, 207, 208],\n",
       " [151, 152, 153, 179, 181, 207, 208, 209],\n",
       " [152, 153, 154, 180, 182, 208, 209, 210],\n",
       " [153, 154, 155, 181, 183, 209, 210, 211],\n",
       " [154, 155, 156, 182, 184, 210, 211, 212],\n",
       " [155, 156, 157, 183, 185, 211, 212, 213],\n",
       " [156, 157, 158, 184, 186, 212, 213, 214],\n",
       " [157, 158, 159, 185, 187, 213, 214, 215],\n",
       " [158, 159, 160, 186, 188, 214, 215, 216],\n",
       " [159, 160, 161, 187, 189, 215, 216, 217],\n",
       " [160, 161, 162, 188, 190, 216, 217, 218],\n",
       " [161, 162, 163, 189, 191, 217, 218, 219],\n",
       " [162, 163, 164, 190, 192, 218, 219, 220],\n",
       " [163, 164, 165, 191, 193, 219, 220, 221],\n",
       " [164, 165, 166, 192, 194, 220, 221, 222],\n",
       " [165, 166, 167, 193, 195, 221, 222, 223],\n",
       " [166, 167, 194, 222, 223],\n",
       " [168, 169, 197, 224, 225],\n",
       " [168, 169, 170, 196, 198, 224, 225, 226],\n",
       " [169, 170, 171, 197, 199, 225, 226, 227],\n",
       " [170, 171, 172, 198, 200, 226, 227, 228],\n",
       " [171, 172, 173, 199, 201, 227, 228, 229],\n",
       " [172, 173, 174, 200, 202, 228, 229, 230],\n",
       " [173, 174, 175, 201, 203, 229, 230, 231],\n",
       " [174, 175, 176, 202, 204, 230, 231, 232],\n",
       " [175, 176, 177, 203, 205, 231, 232, 233],\n",
       " [176, 177, 178, 204, 206, 232, 233, 234],\n",
       " [177, 178, 179, 205, 207, 233, 234, 235],\n",
       " [178, 179, 180, 206, 208, 234, 235, 236],\n",
       " [179, 180, 181, 207, 209, 235, 236, 237],\n",
       " [180, 181, 182, 208, 210, 236, 237, 238],\n",
       " [181, 182, 183, 209, 211, 237, 238, 239],\n",
       " [182, 183, 184, 210, 212, 238, 239, 240],\n",
       " [183, 184, 185, 211, 213, 239, 240, 241],\n",
       " [184, 185, 186, 212, 214, 240, 241, 242],\n",
       " [185, 186, 187, 213, 215, 241, 242, 243],\n",
       " [186, 187, 188, 214, 216, 242, 243, 244],\n",
       " [187, 188, 189, 215, 217, 243, 244, 245],\n",
       " [188, 189, 190, 216, 218, 244, 245, 246],\n",
       " [189, 190, 191, 217, 219, 245, 246, 247],\n",
       " [190, 191, 192, 218, 220, 246, 247, 248],\n",
       " [191, 192, 193, 219, 221, 247, 248, 249],\n",
       " [192, 193, 194, 220, 222, 248, 249, 250],\n",
       " [193, 194, 195, 221, 223, 249, 250, 251],\n",
       " [194, 195, 222, 250, 251],\n",
       " [196, 197, 225, 252, 253],\n",
       " [196, 197, 198, 224, 226, 252, 253, 254],\n",
       " [197, 198, 199, 225, 227, 253, 254, 255],\n",
       " [198, 199, 200, 226, 228, 254, 255, 256],\n",
       " [199, 200, 201, 227, 229, 255, 256, 257],\n",
       " [200, 201, 202, 228, 230, 256, 257, 258],\n",
       " [201, 202, 203, 229, 231, 257, 258, 259],\n",
       " [202, 203, 204, 230, 232, 258, 259, 260],\n",
       " [203, 204, 205, 231, 233, 259, 260, 261],\n",
       " [204, 205, 206, 232, 234, 260, 261, 262],\n",
       " [205, 206, 207, 233, 235, 261, 262, 263],\n",
       " [206, 207, 208, 234, 236, 262, 263, 264],\n",
       " [207, 208, 209, 235, 237, 263, 264, 265],\n",
       " [208, 209, 210, 236, 238, 264, 265, 266],\n",
       " [209, 210, 211, 237, 239, 265, 266, 267],\n",
       " [210, 211, 212, 238, 240, 266, 267, 268],\n",
       " [211, 212, 213, 239, 241, 267, 268, 269],\n",
       " [212, 213, 214, 240, 242, 268, 269, 270],\n",
       " [213, 214, 215, 241, 243, 269, 270, 271],\n",
       " [214, 215, 216, 242, 244, 270, 271, 272],\n",
       " [215, 216, 217, 243, 245, 271, 272, 273],\n",
       " [216, 217, 218, 244, 246, 272, 273, 274],\n",
       " [217, 218, 219, 245, 247, 273, 274, 275],\n",
       " [218, 219, 220, 246, 248, 274, 275, 276],\n",
       " [219, 220, 221, 247, 249, 275, 276, 277],\n",
       " [220, 221, 222, 248, 250, 276, 277, 278],\n",
       " [221, 222, 223, 249, 251, 277, 278, 279],\n",
       " [222, 223, 250, 278, 279],\n",
       " [224, 225, 253, 280, 281],\n",
       " [224, 225, 226, 252, 254, 280, 281, 282],\n",
       " [225, 226, 227, 253, 255, 281, 282, 283],\n",
       " [226, 227, 228, 254, 256, 282, 283, 284],\n",
       " [227, 228, 229, 255, 257, 283, 284, 285],\n",
       " [228, 229, 230, 256, 258, 284, 285, 286],\n",
       " [229, 230, 231, 257, 259, 285, 286, 287],\n",
       " [230, 231, 232, 258, 260, 286, 287, 288],\n",
       " [231, 232, 233, 259, 261, 287, 288, 289],\n",
       " [232, 233, 234, 260, 262, 288, 289, 290],\n",
       " [233, 234, 235, 261, 263, 289, 290, 291],\n",
       " [234, 235, 236, 262, 264, 290, 291, 292],\n",
       " [235, 236, 237, 263, 265, 291, 292, 293],\n",
       " [236, 237, 238, 264, 266, 292, 293, 294],\n",
       " [237, 238, 239, 265, 267, 293, 294, 295],\n",
       " [238, 239, 240, 266, 268, 294, 295, 296],\n",
       " [239, 240, 241, 267, 269, 295, 296, 297],\n",
       " [240, 241, 242, 268, 270, 296, 297, 298],\n",
       " [241, 242, 243, 269, 271, 297, 298, 299],\n",
       " [242, 243, 244, 270, 272, 298, 299, 300],\n",
       " [243, 244, 245, 271, 273, 299, 300, 301],\n",
       " [244, 245, 246, 272, 274, 300, 301, 302],\n",
       " [245, 246, 247, 273, 275, 301, 302, 303],\n",
       " [246, 247, 248, 274, 276, 302, 303, 304],\n",
       " [247, 248, 249, 275, 277, 303, 304, 305],\n",
       " [248, 249, 250, 276, 278, 304, 305, 306],\n",
       " [249, 250, 251, 277, 279, 305, 306, 307],\n",
       " [250, 251, 278, 306, 307],\n",
       " [252, 253, 281, 308, 309],\n",
       " [252, 253, 254, 280, 282, 308, 309, 310],\n",
       " [253, 254, 255, 281, 283, 309, 310, 311],\n",
       " [254, 255, 256, 282, 284, 310, 311, 312],\n",
       " [255, 256, 257, 283, 285, 311, 312, 313],\n",
       " [256, 257, 258, 284, 286, 312, 313, 314],\n",
       " [257, 258, 259, 285, 287, 313, 314, 315],\n",
       " [258, 259, 260, 286, 288, 314, 315, 316],\n",
       " [259, 260, 261, 287, 289, 315, 316, 317],\n",
       " [260, 261, 262, 288, 290, 316, 317, 318],\n",
       " [261, 262, 263, 289, 291, 317, 318, 319],\n",
       " [262, 263, 264, 290, 292, 318, 319, 320],\n",
       " [263, 264, 265, 291, 293, 319, 320, 321],\n",
       " [264, 265, 266, 292, 294, 320, 321, 322],\n",
       " [265, 266, 267, 293, 295, 321, 322, 323],\n",
       " [266, 267, 268, 294, 296, 322, 323, 324],\n",
       " [267, 268, 269, 295, 297, 323, 324, 325],\n",
       " [268, 269, 270, 296, 298, 324, 325, 326],\n",
       " [269, 270, 271, 297, 299, 325, 326, 327],\n",
       " [270, 271, 272, 298, 300, 326, 327, 328],\n",
       " [271, 272, 273, 299, 301, 327, 328, 329],\n",
       " [272, 273, 274, 300, 302, 328, 329, 330],\n",
       " [273, 274, 275, 301, 303, 329, 330, 331],\n",
       " [274, 275, 276, 302, 304, 330, 331, 332],\n",
       " [275, 276, 277, 303, 305, 331, 332, 333],\n",
       " [276, 277, 278, 304, 306, 332, 333, 334],\n",
       " [277, 278, 279, 305, 307, 333, 334, 335],\n",
       " [278, 279, 306, 334, 335],\n",
       " [280, 281, 309, 336, 337],\n",
       " [280, 281, 282, 308, 310, 336, 337, 338],\n",
       " [281, 282, 283, 309, 311, 337, 338, 339],\n",
       " [282, 283, 284, 310, 312, 338, 339, 340],\n",
       " [283, 284, 285, 311, 313, 339, 340, 341],\n",
       " [284, 285, 286, 312, 314, 340, 341, 342],\n",
       " [285, 286, 287, 313, 315, 341, 342, 343],\n",
       " [286, 287, 288, 314, 316, 342, 343, 344],\n",
       " [287, 288, 289, 315, 317, 343, 344, 345],\n",
       " [288, 289, 290, 316, 318, 344, 345, 346],\n",
       " [289, 290, 291, 317, 319, 345, 346, 347],\n",
       " [290, 291, 292, 318, 320, 346, 347, 348],\n",
       " [291, 292, 293, 319, 321, 347, 348, 349],\n",
       " [292, 293, 294, 320, 322, 348, 349, 350],\n",
       " [293, 294, 295, 321, 323, 349, 350, 351],\n",
       " [294, 295, 296, 322, 324, 350, 351, 352],\n",
       " [295, 296, 297, 323, 325, 351, 352, 353],\n",
       " [296, 297, 298, 324, 326, 352, 353, 354],\n",
       " [297, 298, 299, 325, 327, 353, 354, 355],\n",
       " [298, 299, 300, 326, 328, 354, 355, 356],\n",
       " [299, 300, 301, 327, 329, 355, 356, 357],\n",
       " [300, 301, 302, 328, 330, 356, 357, 358],\n",
       " [301, 302, 303, 329, 331, 357, 358, 359],\n",
       " [302, 303, 304, 330, 332, 358, 359, 360],\n",
       " [303, 304, 305, 331, 333, 359, 360, 361],\n",
       " [304, 305, 306, 332, 334, 360, 361, 362],\n",
       " [305, 306, 307, 333, 335, 361, 362, 363],\n",
       " [306, 307, 334, 362, 363],\n",
       " [308, 309, 337, 364, 365],\n",
       " [308, 309, 310, 336, 338, 364, 365, 366],\n",
       " [309, 310, 311, 337, 339, 365, 366, 367],\n",
       " [310, 311, 312, 338, 340, 366, 367, 368],\n",
       " [311, 312, 313, 339, 341, 367, 368, 369],\n",
       " [312, 313, 314, 340, 342, 368, 369, 370],\n",
       " [313, 314, 315, 341, 343, 369, 370, 371],\n",
       " [314, 315, 316, 342, 344, 370, 371, 372],\n",
       " [315, 316, 317, 343, 345, 371, 372, 373],\n",
       " [316, 317, 318, 344, 346, 372, 373, 374],\n",
       " [317, 318, 319, 345, 347, 373, 374, 375],\n",
       " [318, 319, 320, 346, 348, 374, 375, 376],\n",
       " [319, 320, 321, 347, 349, 375, 376, 377],\n",
       " [320, 321, 322, 348, 350, 376, 377, 378],\n",
       " [321, 322, 323, 349, 351, 377, 378, 379],\n",
       " [322, 323, 324, 350, 352, 378, 379, 380],\n",
       " [323, 324, 325, 351, 353, 379, 380, 381],\n",
       " [324, 325, 326, 352, 354, 380, 381, 382],\n",
       " [325, 326, 327, 353, 355, 381, 382, 383],\n",
       " [326, 327, 328, 354, 356, 382, 383, 384],\n",
       " [327, 328, 329, 355, 357, 383, 384, 385],\n",
       " [328, 329, 330, 356, 358, 384, 385, 386],\n",
       " [329, 330, 331, 357, 359, 385, 386, 387],\n",
       " [330, 331, 332, 358, 360, 386, 387, 388],\n",
       " [331, 332, 333, 359, 361, 387, 388, 389],\n",
       " [332, 333, 334, 360, 362, 388, 389, 390],\n",
       " [333, 334, 335, 361, 363, 389, 390, 391],\n",
       " [334, 335, 362, 390, 391],\n",
       " [336, 337, 365, 392, 393],\n",
       " [336, 337, 338, 364, 366, 392, 393, 394],\n",
       " [337, 338, 339, 365, 367, 393, 394, 395],\n",
       " [338, 339, 340, 366, 368, 394, 395, 396],\n",
       " [339, 340, 341, 367, 369, 395, 396, 397],\n",
       " [340, 341, 342, 368, 370, 396, 397, 398],\n",
       " [341, 342, 343, 369, 371, 397, 398, 399],\n",
       " [342, 343, 344, 370, 372, 398, 399, 400],\n",
       " [343, 344, 345, 371, 373, 399, 400, 401],\n",
       " [344, 345, 346, 372, 374, 400, 401, 402],\n",
       " [345, 346, 347, 373, 375, 401, 402, 403],\n",
       " [346, 347, 348, 374, 376, 402, 403, 404],\n",
       " [347, 348, 349, 375, 377, 403, 404, 405],\n",
       " [348, 349, 350, 376, 378, 404, 405, 406],\n",
       " [349, 350, 351, 377, 379, 405, 406, 407],\n",
       " [350, 351, 352, 378, 380, 406, 407, 408],\n",
       " [351, 352, 353, 379, 381, 407, 408, 409],\n",
       " [352, 353, 354, 380, 382, 408, 409, 410],\n",
       " [353, 354, 355, 381, 383, 409, 410, 411],\n",
       " [354, 355, 356, 382, 384, 410, 411, 412],\n",
       " [355, 356, 357, 383, 385, 411, 412, 413],\n",
       " [356, 357, 358, 384, 386, 412, 413, 414],\n",
       " [357, 358, 359, 385, 387, 413, 414, 415],\n",
       " [358, 359, 360, 386, 388, 414, 415, 416],\n",
       " [359, 360, 361, 387, 389, 415, 416, 417],\n",
       " [360, 361, 362, 388, 390, 416, 417, 418],\n",
       " [361, 362, 363, 389, 391, 417, 418, 419],\n",
       " [362, 363, 390, 418, 419],\n",
       " [364, 365, 393, 420, 421],\n",
       " [364, 365, 366, 392, 394, 420, 421, 422],\n",
       " [365, 366, 367, 393, 395, 421, 422, 423],\n",
       " [366, 367, 368, 394, 396, 422, 423, 424],\n",
       " [367, 368, 369, 395, 397, 423, 424, 425],\n",
       " [368, 369, 370, 396, 398, 424, 425, 426],\n",
       " [369, 370, 371, 397, 399, 425, 426, 427],\n",
       " [370, 371, 372, 398, 400, 426, 427, 428],\n",
       " [371, 372, 373, 399, 401, 427, 428, 429],\n",
       " [372, 373, 374, 400, 402, 428, 429, 430],\n",
       " [373, 374, 375, 401, 403, 429, 430, 431],\n",
       " [374, 375, 376, 402, 404, 430, 431, 432],\n",
       " [375, 376, 377, 403, 405, 431, 432, 433],\n",
       " [376, 377, 378, 404, 406, 432, 433, 434],\n",
       " [377, 378, 379, 405, 407, 433, 434, 435],\n",
       " [378, 379, 380, 406, 408, 434, 435, 436],\n",
       " [379, 380, 381, 407, 409, 435, 436, 437],\n",
       " [380, 381, 382, 408, 410, 436, 437, 438],\n",
       " [381, 382, 383, 409, 411, 437, 438, 439],\n",
       " [382, 383, 384, 410, 412, 438, 439, 440],\n",
       " [383, 384, 385, 411, 413, 439, 440, 441],\n",
       " [384, 385, 386, 412, 414, 440, 441, 442],\n",
       " [385, 386, 387, 413, 415, 441, 442, 443],\n",
       " [386, 387, 388, 414, 416, 442, 443, 444],\n",
       " [387, 388, 389, 415, 417, 443, 444, 445],\n",
       " [388, 389, 390, 416, 418, 444, 445, 446],\n",
       " [389, 390, 391, 417, 419, 445, 446, 447],\n",
       " [390, 391, 418, 446, 447],\n",
       " [392, 393, 421, 448, 449],\n",
       " [392, 393, 394, 420, 422, 448, 449, 450],\n",
       " [393, 394, 395, 421, 423, 449, 450, 451],\n",
       " [394, 395, 396, 422, 424, 450, 451, 452],\n",
       " [395, 396, 397, 423, 425, 451, 452, 453],\n",
       " [396, 397, 398, 424, 426, 452, 453, 454],\n",
       " [397, 398, 399, 425, 427, 453, 454, 455],\n",
       " [398, 399, 400, 426, 428, 454, 455, 456],\n",
       " [399, 400, 401, 427, 429, 455, 456, 457],\n",
       " [400, 401, 402, 428, 430, 456, 457, 458],\n",
       " [401, 402, 403, 429, 431, 457, 458, 459],\n",
       " [402, 403, 404, 430, 432, 458, 459, 460],\n",
       " [403, 404, 405, 431, 433, 459, 460, 461],\n",
       " [404, 405, 406, 432, 434, 460, 461, 462],\n",
       " [405, 406, 407, 433, 435, 461, 462, 463],\n",
       " [406, 407, 408, 434, 436, 462, 463, 464],\n",
       " [407, 408, 409, 435, 437, 463, 464, 465],\n",
       " [408, 409, 410, 436, 438, 464, 465, 466],\n",
       " [409, 410, 411, 437, 439, 465, 466, 467],\n",
       " [410, 411, 412, 438, 440, 466, 467, 468],\n",
       " [411, 412, 413, 439, 441, 467, 468, 469],\n",
       " [412, 413, 414, 440, 442, 468, 469, 470],\n",
       " [413, 414, 415, 441, 443, 469, 470, 471],\n",
       " [414, 415, 416, 442, 444, 470, 471, 472],\n",
       " [415, 416, 417, 443, 445, 471, 472, 473],\n",
       " [416, 417, 418, 444, 446, 472, 473, 474],\n",
       " [417, 418, 419, 445, 447, 473, 474, 475],\n",
       " [418, 419, 446, 474, 475],\n",
       " [420, 421, 449, 476, 477],\n",
       " [420, 421, 422, 448, 450, 476, 477, 478],\n",
       " [421, 422, 423, 449, 451, 477, 478, 479],\n",
       " [422, 423, 424, 450, 452, 478, 479, 480],\n",
       " [423, 424, 425, 451, 453, 479, 480, 481],\n",
       " [424, 425, 426, 452, 454, 480, 481, 482],\n",
       " [425, 426, 427, 453, 455, 481, 482, 483],\n",
       " [426, 427, 428, 454, 456, 482, 483, 484],\n",
       " [427, 428, 429, 455, 457, 483, 484, 485],\n",
       " [428, 429, 430, 456, 458, 484, 485, 486],\n",
       " [429, 430, 431, 457, 459, 485, 486, 487],\n",
       " [430, 431, 432, 458, 460, 486, 487, 488],\n",
       " [431, 432, 433, 459, 461, 487, 488, 489],\n",
       " [432, 433, 434, 460, 462, 488, 489, 490],\n",
       " [433, 434, 435, 461, 463, 489, 490, 491],\n",
       " [434, 435, 436, 462, 464, 490, 491, 492],\n",
       " [435, 436, 437, 463, 465, 491, 492, 493],\n",
       " [436, 437, 438, 464, 466, 492, 493, 494],\n",
       " [437, 438, 439, 465, 467, 493, 494, 495],\n",
       " [438, 439, 440, 466, 468, 494, 495, 496],\n",
       " [439, 440, 441, 467, 469, 495, 496, 497],\n",
       " [440, 441, 442, 468, 470, 496, 497, 498],\n",
       " [441, 442, 443, 469, 471, 497, 498, 499],\n",
       " [442, 443, 444, 470, 472, 498, 499, 500],\n",
       " [443, 444, 445, 471, 473, 499, 500, 501],\n",
       " [444, 445, 446, 472, 474, 500, 501, 502],\n",
       " [445, 446, 447, 473, 475, 501, 502, 503],\n",
       " [446, 447, 474, 502, 503],\n",
       " [448, 449, 477, 504, 505],\n",
       " [448, 449, 450, 476, 478, 504, 505, 506],\n",
       " [449, 450, 451, 477, 479, 505, 506, 507],\n",
       " [450, 451, 452, 478, 480, 506, 507, 508],\n",
       " [451, 452, 453, 479, 481, 507, 508, 509],\n",
       " [452, 453, 454, 480, 482, 508, 509, 510],\n",
       " [453, 454, 455, 481, 483, 509, 510, 511],\n",
       " [454, 455, 456, 482, 484, 510, 511, 512],\n",
       " [455, 456, 457, 483, 485, 511, 512, 513],\n",
       " [456, 457, 458, 484, 486, 512, 513, 514],\n",
       " [457, 458, 459, 485, 487, 513, 514, 515],\n",
       " [458, 459, 460, 486, 488, 514, 515, 516],\n",
       " [459, 460, 461, 487, 489, 515, 516, 517],\n",
       " [460, 461, 462, 488, 490, 516, 517, 518],\n",
       " [461, 462, 463, 489, 491, 517, 518, 519],\n",
       " [462, 463, 464, 490, 492, 518, 519, 520],\n",
       " [463, 464, 465, 491, 493, 519, 520, 521],\n",
       " [464, 465, 466, 492, 494, 520, 521, 522],\n",
       " [465, 466, 467, 493, 495, 521, 522, 523],\n",
       " [466, 467, 468, 494, 496, 522, 523, 524],\n",
       " [467, 468, 469, 495, 497, 523, 524, 525],\n",
       " [468, 469, 470, 496, 498, 524, 525, 526],\n",
       " [469, 470, 471, 497, 499, 525, 526, 527],\n",
       " [470, 471, 472, 498, 500, 526, 527, 528],\n",
       " [471, 472, 473, 499, 501, 527, 528, 529],\n",
       " [472, 473, 474, 500, 502, 528, 529, 530],\n",
       " [473, 474, 475, 501, 503, 529, 530, 531],\n",
       " [474, 475, 502, 530, 531],\n",
       " [476, 477, 505, 532, 533],\n",
       " [476, 477, 478, 504, 506, 532, 533, 534],\n",
       " [477, 478, 479, 505, 507, 533, 534, 535],\n",
       " [478, 479, 480, 506, 508, 534, 535, 536],\n",
       " [479, 480, 481, 507, 509, 535, 536, 537],\n",
       " [480, 481, 482, 508, 510, 536, 537, 538],\n",
       " [481, 482, 483, 509, 511, 537, 538, 539],\n",
       " [482, 483, 484, 510, 512, 538, 539, 540],\n",
       " [483, 484, 485, 511, 513, 539, 540, 541],\n",
       " [484, 485, 486, 512, 514, 540, 541, 542],\n",
       " [485, 486, 487, 513, 515, 541, 542, 543],\n",
       " [486, 487, 488, 514, 516, 542, 543, 544],\n",
       " [487, 488, 489, 515, 517, 543, 544, 545],\n",
       " [488, 489, 490, 516, 518, 544, 545, 546],\n",
       " [489, 490, 491, 517, 519, 545, 546, 547],\n",
       " [490, 491, 492, 518, 520, 546, 547, 548],\n",
       " [491, 492, 493, 519, 521, 547, 548, 549],\n",
       " [492, 493, 494, 520, 522, 548, 549, 550],\n",
       " [493, 494, 495, 521, 523, 549, 550, 551],\n",
       " [494, 495, 496, 522, 524, 550, 551, 552],\n",
       " [495, 496, 497, 523, 525, 551, 552, 553],\n",
       " [496, 497, 498, 524, 526, 552, 553, 554],\n",
       " [497, 498, 499, 525, 527, 553, 554, 555],\n",
       " [498, 499, 500, 526, 528, 554, 555, 556],\n",
       " [499, 500, 501, 527, 529, 555, 556, 557],\n",
       " [500, 501, 502, 528, 530, 556, 557, 558],\n",
       " [501, 502, 503, 529, 531, 557, 558, 559],\n",
       " [502, 503, 530, 558, 559],\n",
       " [504, 505, 533, 560, 561],\n",
       " [504, 505, 506, 532, 534, 560, 561, 562],\n",
       " [505, 506, 507, 533, 535, 561, 562, 563],\n",
       " [506, 507, 508, 534, 536, 562, 563, 564],\n",
       " [507, 508, 509, 535, 537, 563, 564, 565],\n",
       " [508, 509, 510, 536, 538, 564, 565, 566],\n",
       " [509, 510, 511, 537, 539, 565, 566, 567],\n",
       " [510, 511, 512, 538, 540, 566, 567, 568],\n",
       " [511, 512, 513, 539, 541, 567, 568, 569],\n",
       " [512, 513, 514, 540, 542, 568, 569, 570],\n",
       " [513, 514, 515, 541, 543, 569, 570, 571],\n",
       " [514, 515, 516, 542, 544, 570, 571, 572],\n",
       " [515, 516, 517, 543, 545, 571, 572, 573],\n",
       " [516, 517, 518, 544, 546, 572, 573, 574],\n",
       " [517, 518, 519, 545, 547, 573, 574, 575],\n",
       " [518, 519, 520, 546, 548, 574, 575, 576],\n",
       " [519, 520, 521, 547, 549, 575, 576, 577],\n",
       " [520, 521, 522, 548, 550, 576, 577, 578],\n",
       " [521, 522, 523, 549, 551, 577, 578, 579],\n",
       " [522, 523, 524, 550, 552, 578, 579, 580],\n",
       " [523, 524, 525, 551, 553, 579, 580, 581],\n",
       " [524, 525, 526, 552, 554, 580, 581, 582],\n",
       " [525, 526, 527, 553, 555, 581, 582, 583],\n",
       " [526, 527, 528, 554, 556, 582, 583, 584],\n",
       " [527, 528, 529, 555, 557, 583, 584, 585],\n",
       " [528, 529, 530, 556, 558, 584, 585, 586],\n",
       " [529, 530, 531, 557, 559, 585, 586, 587],\n",
       " [530, 531, 558, 586, 587],\n",
       " [532, 533, 561, 588, 589],\n",
       " [532, 533, 534, 560, 562, 588, 589, 590],\n",
       " [533, 534, 535, 561, 563, 589, 590, 591],\n",
       " [534, 535, 536, 562, 564, 590, 591, 592],\n",
       " [535, 536, 537, 563, 565, 591, 592, 593],\n",
       " [536, 537, 538, 564, 566, 592, 593, 594],\n",
       " [537, 538, 539, 565, 567, 593, 594, 595],\n",
       " [538, 539, 540, 566, 568, 594, 595, 596],\n",
       " [539, 540, 541, 567, 569, 595, 596, 597],\n",
       " [540, 541, 542, 568, 570, 596, 597, 598],\n",
       " [541, 542, 543, 569, 571, 597, 598, 599],\n",
       " [542, 543, 544, 570, 572, 598, 599, 600],\n",
       " [543, 544, 545, 571, 573, 599, 600, 601],\n",
       " [544, 545, 546, 572, 574, 600, 601, 602],\n",
       " [545, 546, 547, 573, 575, 601, 602, 603],\n",
       " [546, 547, 548, 574, 576, 602, 603, 604],\n",
       " [547, 548, 549, 575, 577, 603, 604, 605],\n",
       " [548, 549, 550, 576, 578, 604, 605, 606],\n",
       " [549, 550, 551, 577, 579, 605, 606, 607],\n",
       " [550, 551, 552, 578, 580, 606, 607, 608],\n",
       " [551, 552, 553, 579, 581, 607, 608, 609],\n",
       " [552, 553, 554, 580, 582, 608, 609, 610],\n",
       " [553, 554, 555, 581, 583, 609, 610, 611],\n",
       " [554, 555, 556, 582, 584, 610, 611, 612],\n",
       " [555, 556, 557, 583, 585, 611, 612, 613],\n",
       " [556, 557, 558, 584, 586, 612, 613, 614],\n",
       " [557, 558, 559, 585, 587, 613, 614, 615],\n",
       " [558, 559, 586, 614, 615],\n",
       " [560, 561, 589, 616, 617],\n",
       " [560, 561, 562, 588, 590, 616, 617, 618],\n",
       " [561, 562, 563, 589, 591, 617, 618, 619],\n",
       " [562, 563, 564, 590, 592, 618, 619, 620],\n",
       " [563, 564, 565, 591, 593, 619, 620, 621],\n",
       " [564, 565, 566, 592, 594, 620, 621, 622],\n",
       " [565, 566, 567, 593, 595, 621, 622, 623],\n",
       " [566, 567, 568, 594, 596, 622, 623, 624],\n",
       " [567, 568, 569, 595, 597, 623, 624, 625],\n",
       " [568, 569, 570, 596, 598, 624, 625, 626],\n",
       " [569, 570, 571, 597, 599, 625, 626, 627],\n",
       " [570, 571, 572, 598, 600, 626, 627, 628],\n",
       " [571, 572, 573, 599, 601, 627, 628, 629],\n",
       " [572, 573, 574, 600, 602, 628, 629, 630],\n",
       " [573, 574, 575, 601, 603, 629, 630, 631],\n",
       " [574, 575, 576, 602, 604, 630, 631, 632],\n",
       " [575, 576, 577, 603, 605, 631, 632, 633],\n",
       " [576, 577, 578, 604, 606, 632, 633, 634],\n",
       " [577, 578, 579, 605, 607, 633, 634, 635],\n",
       " [578, 579, 580, 606, 608, 634, 635, 636],\n",
       " [579, 580, 581, 607, 609, 635, 636, 637],\n",
       " [580, 581, 582, 608, 610, 636, 637, 638],\n",
       " [581, 582, 583, 609, 611, 637, 638, 639],\n",
       " [582, 583, 584, 610, 612, 638, 639, 640],\n",
       " [583, 584, 585, 611, 613, 639, 640, 641],\n",
       " [584, 585, 586, 612, 614, 640, 641, 642],\n",
       " [585, 586, 587, 613, 615, 641, 642, 643],\n",
       " [586, 587, 614, 642, 643],\n",
       " [588, 589, 617, 644, 645],\n",
       " [588, 589, 590, 616, 618, 644, 645, 646],\n",
       " [589, 590, 591, 617, 619, 645, 646, 647],\n",
       " [590, 591, 592, 618, 620, 646, 647, 648],\n",
       " [591, 592, 593, 619, 621, 647, 648, 649],\n",
       " [592, 593, 594, 620, 622, 648, 649, 650],\n",
       " [593, 594, 595, 621, 623, 649, 650, 651],\n",
       " [594, 595, 596, 622, 624, 650, 651, 652],\n",
       " [595, 596, 597, 623, 625, 651, 652, 653],\n",
       " [596, 597, 598, 624, 626, 652, 653, 654],\n",
       " [597, 598, 599, 625, 627, 653, 654, 655],\n",
       " [598, 599, 600, 626, 628, 654, 655, 656],\n",
       " [599, 600, 601, 627, 629, 655, 656, 657],\n",
       " [600, 601, 602, 628, 630, 656, 657, 658],\n",
       " [601, 602, 603, 629, 631, 657, 658, 659],\n",
       " [602, 603, 604, 630, 632, 658, 659, 660],\n",
       " [603, 604, 605, 631, 633, 659, 660, 661],\n",
       " [604, 605, 606, 632, 634, 660, 661, 662],\n",
       " [605, 606, 607, 633, 635, 661, 662, 663],\n",
       " [606, 607, 608, 634, 636, 662, 663, 664],\n",
       " [607, 608, 609, 635, 637, 663, 664, 665],\n",
       " [608, 609, 610, 636, 638, 664, 665, 666],\n",
       " [609, 610, 611, 637, 639, 665, 666, 667],\n",
       " [610, 611, 612, 638, 640, 666, 667, 668],\n",
       " [611, 612, 613, 639, 641, 667, 668, 669],\n",
       " [612, 613, 614, 640, 642, 668, 669, 670],\n",
       " [613, 614, 615, 641, 643, 669, 670, 671],\n",
       " [614, 615, 642, 670, 671],\n",
       " [616, 617, 645, 672, 673],\n",
       " [616, 617, 618, 644, 646, 672, 673, 674],\n",
       " [617, 618, 619, 645, 647, 673, 674, 675],\n",
       " [618, 619, 620, 646, 648, 674, 675, 676],\n",
       " [619, 620, 621, 647, 649, 675, 676, 677],\n",
       " [620, 621, 622, 648, 650, 676, 677, 678],\n",
       " [621, 622, 623, 649, 651, 677, 678, 679],\n",
       " [622, 623, 624, 650, 652, 678, 679, 680],\n",
       " [623, 624, 625, 651, 653, 679, 680, 681],\n",
       " [624, 625, 626, 652, 654, 680, 681, 682],\n",
       " [625, 626, 627, 653, 655, 681, 682, 683],\n",
       " [626, 627, 628, 654, 656, 682, 683, 684],\n",
       " [627, 628, 629, 655, 657, 683, 684, 685],\n",
       " [628, 629, 630, 656, 658, 684, 685, 686],\n",
       " [629, 630, 631, 657, 659, 685, 686, 687],\n",
       " [630, 631, 632, 658, 660, 686, 687, 688],\n",
       " [631, 632, 633, 659, 661, 687, 688, 689],\n",
       " [632, 633, 634, 660, 662, 688, 689, 690],\n",
       " [633, 634, 635, 661, 663, 689, 690, 691],\n",
       " [634, 635, 636, 662, 664, 690, 691, 692],\n",
       " [635, 636, 637, 663, 665, 691, 692, 693],\n",
       " [636, 637, 638, 664, 666, 692, 693, 694],\n",
       " [637, 638, 639, 665, 667, 693, 694, 695],\n",
       " [638, 639, 640, 666, 668, 694, 695, 696],\n",
       " [639, 640, 641, 667, 669, 695, 696, 697],\n",
       " [640, 641, 642, 668, 670, 696, 697, 698],\n",
       " [641, 642, 643, 669, 671, 697, 698, 699],\n",
       " [642, 643, 670, 698, 699],\n",
       " [644, 645, 673, 700, 701],\n",
       " [644, 645, 646, 672, 674, 700, 701, 702],\n",
       " [645, 646, 647, 673, 675, 701, 702, 703],\n",
       " [646, 647, 648, 674, 676, 702, 703, 704],\n",
       " [647, 648, 649, 675, 677, 703, 704, 705],\n",
       " [648, 649, 650, 676, 678, 704, 705, 706],\n",
       " [649, 650, 651, 677, 679, 705, 706, 707],\n",
       " [650, 651, 652, 678, 680, 706, 707, 708],\n",
       " [651, 652, 653, 679, 681, 707, 708, 709],\n",
       " [652, 653, 654, 680, 682, 708, 709, 710],\n",
       " [653, 654, 655, 681, 683, 709, 710, 711],\n",
       " [654, 655, 656, 682, 684, 710, 711, 712],\n",
       " [655, 656, 657, 683, 685, 711, 712, 713],\n",
       " [656, 657, 658, 684, 686, 712, 713, 714],\n",
       " [657, 658, 659, 685, 687, 713, 714, 715],\n",
       " [658, 659, 660, 686, 688, 714, 715, 716],\n",
       " [659, 660, 661, 687, 689, 715, 716, 717],\n",
       " [660, 661, 662, 688, 690, 716, 717, 718],\n",
       " [661, 662, 663, 689, 691, 717, 718, 719],\n",
       " [662, 663, 664, 690, 692, 718, 719, 720],\n",
       " [663, 664, 665, 691, 693, 719, 720, 721],\n",
       " [664, 665, 666, 692, 694, 720, 721, 722],\n",
       " [665, 666, 667, 693, 695, 721, 722, 723],\n",
       " [666, 667, 668, 694, 696, 722, 723, 724],\n",
       " [667, 668, 669, 695, 697, 723, 724, 725],\n",
       " [668, 669, 670, 696, 698, 724, 725, 726],\n",
       " [669, 670, 671, 697, 699, 725, 726, 727],\n",
       " [670, 671, 698, 726, 727],\n",
       " [672, 673, 701, 728, 729],\n",
       " [672, 673, 674, 700, 702, 728, 729, 730],\n",
       " [673, 674, 675, 701, 703, 729, 730, 731],\n",
       " [674, 675, 676, 702, 704, 730, 731, 732],\n",
       " [675, 676, 677, 703, 705, 731, 732, 733],\n",
       " [676, 677, 678, 704, 706, 732, 733, 734],\n",
       " [677, 678, 679, 705, 707, 733, 734, 735],\n",
       " [678, 679, 680, 706, 708, 734, 735, 736],\n",
       " [679, 680, 681, 707, 709, 735, 736, 737],\n",
       " [680, 681, 682, 708, 710, 736, 737, 738],\n",
       " [681, 682, 683, 709, 711, 737, 738, 739],\n",
       " [682, 683, 684, 710, 712, 738, 739, 740],\n",
       " [683, 684, 685, 711, 713, 739, 740, 741],\n",
       " [684, 685, 686, 712, 714, 740, 741, 742],\n",
       " [685, 686, 687, 713, 715, 741, 742, 743],\n",
       " [686, 687, 688, 714, 716, 742, 743, 744],\n",
       " [687, 688, 689, 715, 717, 743, 744, 745],\n",
       " [688, 689, 690, 716, 718, 744, 745, 746],\n",
       " [689, 690, 691, 717, 719, 745, 746, 747],\n",
       " [690, 691, 692, 718, 720, 746, 747, 748],\n",
       " [691, 692, 693, 719, 721, 747, 748, 749],\n",
       " [692, 693, 694, 720, 722, 748, 749, 750],\n",
       " [693, 694, 695, 721, 723, 749, 750, 751],\n",
       " [694, 695, 696, 722, 724, 750, 751, 752],\n",
       " [695, 696, 697, 723, 725, 751, 752, 753],\n",
       " [696, 697, 698, 724, 726, 752, 753, 754],\n",
       " [697, 698, 699, 725, 727, 753, 754, 755],\n",
       " [698, 699, 726, 754, 755],\n",
       " [700, 701, 729, 756, 757],\n",
       " [700, 701, 702, 728, 730, 756, 757, 758],\n",
       " [701, 702, 703, 729, 731, 757, 758, 759],\n",
       " [702, 703, 704, 730, 732, 758, 759, 760],\n",
       " [703, 704, 705, 731, 733, 759, 760, 761],\n",
       " [704, 705, 706, 732, 734, 760, 761, 762],\n",
       " [705, 706, 707, 733, 735, 761, 762, 763],\n",
       " [706, 707, 708, 734, 736, 762, 763, 764],\n",
       " [707, 708, 709, 735, 737, 763, 764, 765],\n",
       " [708, 709, 710, 736, 738, 764, 765, 766],\n",
       " [709, 710, 711, 737, 739, 765, 766, 767],\n",
       " [710, 711, 712, 738, 740, 766, 767, 768],\n",
       " [711, 712, 713, 739, 741, 767, 768, 769],\n",
       " [712, 713, 714, 740, 742, 768, 769, 770],\n",
       " [713, 714, 715, 741, 743, 769, 770, 771],\n",
       " [714, 715, 716, 742, 744, 770, 771, 772],\n",
       " [715, 716, 717, 743, 745, 771, 772, 773],\n",
       " [716, 717, 718, 744, 746, 772, 773, 774],\n",
       " [717, 718, 719, 745, 747, 773, 774, 775],\n",
       " [718, 719, 720, 746, 748, 774, 775, 776],\n",
       " [719, 720, 721, 747, 749, 775, 776, 777],\n",
       " [720, 721, 722, 748, 750, 776, 777, 778],\n",
       " [721, 722, 723, 749, 751, 777, 778, 779],\n",
       " [722, 723, 724, 750, 752, 778, 779, 780],\n",
       " [723, 724, 725, 751, 753, 779, 780, 781],\n",
       " [724, 725, 726, 752, 754, 780, 781, 782],\n",
       " [725, 726, 727, 753, 755, 781, 782, 783],\n",
       " [726, 727, 754, 782, 783],\n",
       " [728, 729, 757],\n",
       " [728, 729, 730, 756, 758],\n",
       " [729, 730, 731, 757, 759],\n",
       " [730, 731, 732, 758, 760],\n",
       " [731, 732, 733, 759, 761],\n",
       " [732, 733, 734, 760, 762],\n",
       " [733, 734, 735, 761, 763],\n",
       " [734, 735, 736, 762, 764],\n",
       " [735, 736, 737, 763, 765],\n",
       " [736, 737, 738, 764, 766],\n",
       " [737, 738, 739, 765, 767],\n",
       " [738, 739, 740, 766, 768],\n",
       " [739, 740, 741, 767, 769],\n",
       " [740, 741, 742, 768, 770],\n",
       " [741, 742, 743, 769, 771],\n",
       " [742, 743, 744, 770, 772],\n",
       " [743, 744, 745, 771, 773],\n",
       " [744, 745, 746, 772, 774],\n",
       " [745, 746, 747, 773, 775],\n",
       " [746, 747, 748, 774, 776],\n",
       " [747, 748, 749, 775, 777],\n",
       " [748, 749, 750, 776, 778],\n",
       " [749, 750, 751, 777, 779],\n",
       " [750, 751, 752, 778, 780],\n",
       " [751, 752, 753, 779, 781],\n",
       " [752, 753, 754, 780, 782],\n",
       " [753, 754, 755, 781, 783],\n",
       " [754, 755, 782]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-a3735f7c57bf>\", line 3, in <module>\n",
      "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 42, in __new__\n",
      "    return model.Var(name, dist, data, total_size)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 816, in Var\n",
      "    model=self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 1492, in __init__\n",
      "    transformed_name, transform.apply(distribution), total_size=total_size)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/transforms.py\", line 95, in apply\n",
      "    return TransformedDistribution.dist(dist, self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 52, in dist\n",
      "    dist.__init__(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/transforms.py\", line 125, in __init__\n",
      "    v = forward(FreeRV(name='v', distribution=dist))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 1212, in __init__\n",
      "    self.logp_sum_unscaledt = distribution.logp_sum(self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 119, in logp_sum\n",
      "    return tt.sum(self.logp(*args, **kwargs))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/continuous.py\", line 2355, in logp\n",
      "    beta > 0)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/dist_math.py\", line 51, in bound\n",
      "    return tt.switch(alltrue(conditions), logp, -np.inf)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/dist_math.py\", line 57, in alltrue_elemwise\n",
      "    ret = ret * (1 * c)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/var.py\", line 233, in __rmul__\n",
      "    return theano.tensor.basic.mul(other, self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/gof/op.py\", line 615, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/elemwise.py\", line 482, in make_node\n",
      "    DimShuffle, *inputs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/elemwise.py\", line 424, in get_output_info\n",
      "    for i in inputs])\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 1044, in make_node\n",
      "    for input in inputs])]\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 1052, in output_types\n",
      "    variables = self.output_types_preference(*types)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 838, in upcast_out\n",
      "    dtype = Scalar.upcast(*types)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 420, in upcast\n",
      "    return upcast(*[x.dtype for x in [self] + list(others)])\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 78, in upcast\n",
      "    z = z + make_array(dt=dt)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    beta0  = pm.Normal('beta0', mu=0., tau=1e-2)\n",
    "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
    "    mu_phi = CAR2('mu_phi', w=wmat2, a=amat2, tau=tau, shape=N)\n",
    "    phi    = pm.Deterministic('phi', mu_phi-tt.mean(mu_phi)) # zero-center phi\n",
    "    \n",
    "    \n",
    "    mu = pm.Deterministic('mu', beta0 + phi)\n",
    "    Yi = pm.LogitNormal('Yi', mu=mu, observed=pad(O))\n",
    "    \n",
    "    trace = pm.sample(draws=1000, chains=2)\n",
    "    #trace = pm.sample(draws=2000, step=pm.Metropolis())\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check average proportion of pixels equal to 0 and 1 (or near 1, i.e. 253 or 254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pm.traceplot(trace, varnames=['beta0', 'tau_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.mean(trace.get_values('phi'), axis=0).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.std(trace.get_values('phi'), axis=0).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x_train[12].reshape(-1), bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying breast cancer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread('images/malignant_R_MLO.jpg')\n",
    "image = image[:, :, 0]\n",
    "image = np.max(image) - image\n",
    "x_dim, y_dim = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "position_matrix = np.linspace(0, x_dim*y_dim - 1, num=x_dim*y_dim).astype(np.int64).reshape(x_dim, y_dim)\n",
    "count = 0\n",
    "\n",
    "for i, row in enumerate(position_matrix):\n",
    "    for j, col in enumerate(position_matrix[i]):\n",
    "        assert position_matrix[i][j] == col\n",
    "        \n",
    "        temp = []\n",
    "\n",
    "        # change these loops if we do not want to\n",
    "        # include diagonal elements in adj matrix\n",
    "        for delta_i in [-1, 0, 1]:\n",
    "            for delta_j in [-1, 0, 1]:\n",
    "                if ((i + delta_i) // x_dim == 0) and ((j + delta_j) // y_dim == 0):    \n",
    "                    temp.append(position_matrix[i + delta_i][j + delta_j])\n",
    "        \n",
    "\n",
    "        temp.remove(col)\n",
    "        temp.sort()\n",
    "        adj.append(temp)\n",
    "        \n",
    "weights = [list(np.ones_like(adj_elems).astype(np.int64)) for adj_elems in adj]\n",
    "\n",
    "# below is taken from the pymc3 CAR tutorial website\n",
    "maxwz = max([sum(w) for w in weights])\n",
    "N = len(weights)\n",
    "wmat = np.zeros((N, N))\n",
    "amat = np.zeros((N, N), dtype='int32')\n",
    "for i, a in enumerate(adj):\n",
    "    amat[i, a] = 1\n",
    "    wmat[i, a] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.zeros((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as a:\n",
    "    b = pm.Gamma('test', alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    beta0  = pm.Normal('beta0', mu=0., tau=1e-2)\n",
    "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
    "    mu_phi = CAR2('mu_phi', w=wmat, a=amat, tau=tau, shape=N)\n",
    "    phi    = pm.Deterministic('phi', mu_phi-tt.mean(mu_phi)) # zero-center phi\n",
    "    \n",
    "    \n",
    "    mu = pm.Deterministic('mu', beta0 + phi)\n",
    "    Yi = pm.LogitNormal('Yi', mu=mu, observed=pad(O))\n",
    "    \n",
    "    trace = pm.sample(draws=1000, chains=2)\n",
    "    #trace = pm.sample(draws=2000, step=pm.Metropolis())\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name(name='phi_values', suffix='.npy', directory='results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact sparse CAR models in Stan\n",
    "\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.210407.svg)](https://doi.org/10.5281/zenodo.210407)\n",
    "\n",
    "\n",
    "\n",
    "Max Joseph  \n",
    "August 20, 2016  \n",
    "\n",
    "\n",
    "This document details sparse exact conditional autoregressive (CAR) models in Stan as an extension of previous work on approximate sparse CAR models in Stan. \n",
    "Sparse representations seem to give order of magnitude efficiency gains, scaling better for large spatial data sets. \n",
    "\n",
    "## CAR priors for spatial random effects\n",
    "\n",
    "Conditional autoregressive (CAR) models are popular as prior distributions for spatial random effects with areal spatial data. \n",
    "If we have a random quantity $\\phi = (\\phi_1, \\phi_2, ..., \\phi_n)'$ at $n$ areal locations, the CAR model is often expressed via full conditional distributions:\n",
    "\n",
    "$$\\phi_i \\mid \\phi_j, j \\neq i \\sim N(\\alpha \\sum_{j = 1}^n b_{ij} \\phi_j, \\tau_i^{-1})$$\n",
    "\n",
    "where $\\tau_i$ is a spatially varying precision parameter, and $b_{ii} = 0$. \n",
    "\n",
    "By Brook's Lemma, the joint distribution of $\\phi$ is then:\n",
    "\n",
    "$$\\phi \\sim N(0, [D_\\tau (I - \\alpha B)]^{-1}).$$\n",
    "\n",
    "If we assume the following:\n",
    "\n",
    "- $D_\\tau = \\tau D$\n",
    "- $D = diag(m_i)$: an $n \\times n$ diagonal matrix with $m_i$ = the number of neighbors for location $i$\n",
    "- $I$: an $n \\times n$ identity matrix\n",
    "- $\\alpha$: a parameter that controls spatial dependence ($\\alpha = 0$ implies spatial independence, and $\\alpha = 1$ collapses to an *intrisnic conditional autoregressive* (IAR) specification)\n",
    "- $B = D^{-1} W$: the scaled adjacency matrix\n",
    "- $W$: the adjacency matrix ($w_{ii} = 0, w_{ij} = 1$ if $i$ is a neighbor of $j$, and $w_{ij}=0$ otherwise)\n",
    "\n",
    "then the CAR prior specification simplifies to: \n",
    "\n",
    "$$\\phi \\sim N(0, [\\tau (D - \\alpha W)]^{-1}).$$\n",
    "\n",
    "The $\\alpha$ parameter ensures propriety of the joint distrbution of $\\phi$ as long as $| \\alpha | < 1$ (Gelfand & Vounatsou 2003).\n",
    "However, $\\alpha$ is often taken as 1, leading to the IAR specification which creates a singular precision matrix and an improper prior distribution.\n",
    "\n",
    "## A Poisson specification\n",
    "\n",
    "Suppose we have aggregated count data $y_1, y_2, ..., y_n$ at $n$ locations, and we expect that neighboring locations will have similar counts. \n",
    "With a Poisson likelihood: \n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\text{exp}(X_{i} \\beta + \\phi_i + \\log(\\text{offset}_i)))$$\n",
    "\n",
    "where $X_i$ is a design vector (the $i^{th}$ row from a design matrix), $\\beta$ is a vector of coefficients, $\\phi_i$ is a spatial adjustment, and $\\log(\\text{offset}_i)$ accounts for differences in expected values or exposures at the spatial units (popular choices include area for physical processes, or population size for disease applications). \n",
    "\n",
    "If we specify a proper CAR prior for $\\phi$, then we have that $\\phi \\sim \\text{N}(0, [\\tau (D - \\alpha W)]^{-1})$ where $\\tau (D - \\alpha W)$ is the precision matrix $\\Sigma^{-1}$.\n",
    "A complete Bayesian specification would include priors for the remaining parameters $\\alpha$, $\\tau$, and $\\beta$, such that our posterior distribution is: \n",
    "\n",
    "$$p(\\phi, \\beta, \\alpha, \\tau \\mid y) \\propto p(y \\mid \\beta, \\phi) p(\\phi \\mid \\alpha, \\tau) p(\\alpha) p(\\tau) p(\\beta)$$\n",
    "\n",
    "## Example: Scottish lip cancer data\n",
    "\n",
    "To demonstrate this approach we'll use the Scottish lip cancer data example (some documentation [here](https://cran.r-project.org/web/packages/CARBayesdata/CARBayesdata.pdf)). \n",
    "This data set includes observed lip cancer case counts at 56 spatial units in Scotland, with an expected number of cases to be used as an offset, and an area-specific continuous covariate that represents the proportion of the population employed in agriculture, fishing, or forestry.\n",
    "The model structure is identical to the Poisson model outlined above. \n",
    "\n",
    "\n",
    "```\n",
    "## Warning in gpclibPermit(): support for gpclib will be withdrawn from\n",
    "## maptools at the next major release\n",
    "```\n",
    "\n",
    "```\n",
    "## [1] TRUE\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/make-scotland-map-1.png)<!-- -->\n",
    "\n",
    "Let's start by loading packages and data, specifying the number of MCMC iterations and chains.\n",
    "\n",
    "\n",
    "```r\n",
    "library(ggmcmc)\n",
    "library(rstan)\n",
    "rstan_options(auto_write = TRUE)\n",
    "options(mc.cores = parallel::detectCores())\n",
    "source('data/scotland_lip_cancer.RData')\n",
    "\n",
    "# Define MCMC parameters \n",
    "niter <- 1E4   # definitely overkill, but good for comparison\n",
    "nchains <- 4\n",
    "```\n",
    "\n",
    "To fit the full model, we'll pull objects loaded with our Scotland lip cancer data. \n",
    "I'll use `model.matrix` to generate a design matrix, centering and scaling the continuous covariate `x` to reduce correlation between the intercept and slope estimates. \n",
    "\n",
    "\n",
    "```r\n",
    "W <- A # adjacency matrix\n",
    "scaled_x <- c(scale(x))\n",
    "X <- model.matrix(~scaled_x)\n",
    "  \n",
    "full_d <- list(n = nrow(X),         # number of observations\n",
    "               p = ncol(X),         # number of coefficients\n",
    "               X = X,               # design matrix\n",
    "               y = O,               # observed number of cases\n",
    "               log_offset = log(E), # log(expected) num. cases\n",
    "               W = W)               # adjacency matrix\n",
    "```\n",
    "\n",
    "#### Stan implementation: CAR with `multi_normal_prec`\n",
    "\n",
    "Our model statement mirrors the structure outlined above, with explicit normal and gamma priors on $\\beta$ and $\\tau$ respectively, and a $\\text{Uniform}(0, 1)$ prior for $\\alpha$. \n",
    "The prior on $\\phi$ is specified via the `multi_normal_prec` function, passing in $\\tau (D - \\alpha W)$ as the precision matrix.\n",
    "\n",
    "\n",
    "```\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W;\n",
    "}\n",
    "transformed data{\n",
    "  vector[n] zeros;\n",
    "  matrix<lower = 0>[n, n] D;\n",
    "  {\n",
    "    vector[n] W_rowsums;\n",
    "    for (i in 1:n) {\n",
    "      W_rowsums[i] = sum(W[i, ]);\n",
    "    }\n",
    "    D = diag_matrix(W_rowsums);\n",
    "  }\n",
    "  zeros = rep_vector(0, n);\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi;\n",
    "  real<lower = 0> tau;\n",
    "  real<lower = 0, upper = 1> alpha;\n",
    "}\n",
    "model {\n",
    "  phi ~ multi_normal_prec(zeros, tau * (D - alpha * W));\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "Fitting the model with `rstan`:\n",
    "\n",
    "\n",
    "```r\n",
    "full_fit <- stan('stan/car_prec.stan', data = full_d, \n",
    "                 iter = niter, chains = nchains, verbose = FALSE)\n",
    "print(full_fit, pars = c('beta', 'tau', 'alpha', 'lp__'))\n",
    "```\n",
    "\n",
    "```\n",
    "## Inference for Stan model: car_prec.\n",
    "## 4 chains, each with iter=10000; warmup=5000; thin=1; \n",
    "## post-warmup draws per chain=5000, total post-warmup draws=20000.\n",
    "## \n",
    "##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n",
    "## beta[1]   0.02    0.02 0.29  -0.52  -0.15   0.00   0.16   0.69   321 1.01\n",
    "## beta[2]   0.27    0.00 0.09   0.08   0.21   0.27   0.34   0.45  3981 1.00\n",
    "## tau       1.65    0.01 0.50   0.85   1.29   1.59   1.93   2.83  6218 1.00\n",
    "## alpha     0.93    0.00 0.06   0.77   0.91   0.95   0.98   1.00  3804 1.00\n",
    "## lp__    820.81    0.10 6.73 806.63 816.45 821.18 825.52 832.99  4485 1.00\n",
    "## \n",
    "## Samples were drawn using NUTS(diag_e) at Thu Feb  9 18:25:48 2017.\n",
    "## For each parameter, n_eff is a crude measure of effective sample size,\n",
    "## and Rhat is the potential scale reduction factor on split chains (at \n",
    "## convergence, Rhat=1).\n",
    "```\n",
    "\n",
    "```r\n",
    "# visualize results \n",
    "to_plot <- c('beta', 'tau', 'alpha', 'phi[1]', 'phi[2]', 'phi[3]', 'lp__')\n",
    "traceplot(full_fit, pars = to_plot)\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/fit-prec-model-1.png)<!-- -->\n",
    "\n",
    "### A more efficient sparse representation\n",
    "\n",
    "Although we could specify our multivariate normal prior for $\\phi$ directly in Stan via `multi_normal_prec`, as we did above, in this case we will accrue computational efficiency gains by manually specifying $p(\\phi \\mid \\tau, \\alpha)$ directly via the log probability accumulator. \n",
    "The log probability of $\\phi$ is: \n",
    "\n",
    "$$\\log(p(\\phi \\mid \\tau, \\alpha)) = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\text{det}( \\Sigma^{-1})) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "In Stan, we only need the log posterior up to an additive constant so we can drop the first term. \n",
    "Then, substituting  $\\tau (D - \\alpha W)$ for $\\Sigma^{-1}$:\n",
    "\n",
    "$$\\frac{1}{2} \\log(\\text{det}(\\tau (D - \\alpha W))) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$ = \\frac{1}{2} \\log(\\tau ^ n \\text{det}(D - \\alpha W)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$ = \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(\\text{det}(D - \\alpha W)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "There are two ways that we can accrue computational efficiency gains: \n",
    "\n",
    "1. Sparse representations of $\\Sigma^{-1}$ to expedite computation of $\\phi^T \\Sigma^{-1} \\phi$ (this work was done by Kyle foreman previously, e.g., https://groups.google.com/d/topic/stan-users/M7T7EIlyhoo/discussion). \n",
    "\n",
    "2. Efficient computation of the determinant. Jin, Carlin, and Banerjee (2005) show that:\n",
    "\n",
    "$$\\text{det}(D - \\alpha W) \\propto \\prod_{i = 1}^n (1 - \\alpha \\lambda_i)$$\n",
    "\n",
    "where $\\lambda_1, ..., \\lambda_n$ are the eigenvalues of $D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}$, which can be computed ahead of time and passed in as data. \n",
    "Because we only need the log posterior up to an additive constant, we can use this result which is proportional up to some multiplicative constant $c$: \n",
    "\n",
    "$$\\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(c \\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$= \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(c) +  \\frac{1}{2} \\log(\\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "Again dropping additive constants: \n",
    "\n",
    "$$\\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(\\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$= \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\sum_{i = 1}^n \\log(1 - \\alpha \\lambda_i) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "### Stan implementation: sparse CAR\n",
    "\n",
    "In the Stan model statement's `transformed data` block, we compute $\\lambda_1, ..., \\lambda_n$ (the eigenvalues of $D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}$), and generate a sparse representation for W (`Wsparse`), which is assumed to be symmetric, such that the adjacency relationships can be represented in a two column matrix where each row is an adjacency relationship between two sites. \n",
    "\n",
    "The Stan model statement for the sparse implementation never constructs the precision matrix, and does not call any of the `multi_normal*` functions. \n",
    "Instead, we use define a `sparse_car_lpdf()` function and use it in the model block. \n",
    "\n",
    "\n",
    "```\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper conditional autoregressive (CAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a CAR prior\n",
    "  * @param tau Precision parameter for the CAR prior (real)\n",
    "  * @param alpha Dependence (usually spatial) parameter for the CAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of CAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_car_lpdf(vector phi, real tau, real alpha, \n",
    "    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {\n",
    "      row_vector[n] phit_D; // phi' * D\n",
    "      row_vector[n] phit_W; // phi' * W\n",
    "      vector[n] ldet_terms;\n",
    "    \n",
    "      phit_D = (phi .* D_sparse)';\n",
    "      phit_W = rep_row_vector(0, n);\n",
    "      for (i in 1:W_n) {\n",
    "        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "      }\n",
    "    \n",
    "      for (i in 1:n) ldet_terms[i] = log1m(alpha * lambda[i]);\n",
    "      return 0.5 * (n * log(tau)\n",
    "                    + sum(ldet_terms)\n",
    "                    - tau * (phit_D * phi - alpha * (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W; // adjacency matrix\n",
    "  int W_n;                // number of adjacent region pairs\n",
    "}\n",
    "transformed data {\n",
    "  int W_sparse[W_n, 2];   // adjacency pairs\n",
    "  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)\n",
    "  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  { // generate sparse representation for W\n",
    "  int counter;\n",
    "  counter = 1;\n",
    "  // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1:(n - 1)) {\n",
    "      for (j in (i + 1):n) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1:n) D_sparse[i] = sum(W[i]);\n",
    "  {\n",
    "    vector[n] invsqrtD;  \n",
    "    for (i in 1:n) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi;\n",
    "  real<lower = 0> tau;\n",
    "  real<lower = 0, upper = 1> alpha;\n",
    "}\n",
    "model {\n",
    "  phi ~ sparse_car(tau, alpha, W_sparse, D_sparse, lambda, n, W_n);\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "Fitting the model:\n",
    "\n",
    "\n",
    "```r\n",
    "sp_d <- list(n = nrow(X),         # number of observations\n",
    "             p = ncol(X),         # number of coefficients\n",
    "             X = X,               # design matrix\n",
    "             y = O,               # observed number of cases\n",
    "             log_offset = log(E), # log(expected) num. cases\n",
    "             W_n = sum(W) / 2,    # number of neighbor pairs\n",
    "             W = W)               # adjacency matrix\n",
    "\n",
    "sp_fit <- stan('stan/car_sparse.stan', data = sp_d, \n",
    "               iter = niter, chains = nchains, verbose = FALSE)\n",
    "\n",
    "print(sp_fit, pars = c('beta', 'tau', 'alpha', 'lp__'))\n",
    "```\n",
    "\n",
    "```\n",
    "## Inference for Stan model: car_sparse.\n",
    "## 4 chains, each with iter=10000; warmup=5000; thin=1; \n",
    "## post-warmup draws per chain=5000, total post-warmup draws=20000.\n",
    "## \n",
    "##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n",
    "## beta[1]  -0.01    0.02 0.29  -0.63  -0.15   0.00   0.15   0.57   140 1.03\n",
    "## beta[2]   0.27    0.00 0.09   0.09   0.21   0.27   0.34   0.46  4449 1.00\n",
    "## tau       1.64    0.01 0.50   0.86   1.29   1.58   1.94   2.79  5808 1.00\n",
    "## alpha     0.93    0.00 0.06   0.76   0.91   0.95   0.97   0.99  3169 1.00\n",
    "## lp__    782.96    0.10 6.83 768.65 778.50 783.31 787.71 795.26  4418 1.00\n",
    "## \n",
    "## Samples were drawn using NUTS(diag_e) at Thu Feb  9 18:26:06 2017.\n",
    "## For each parameter, n_eff is a crude measure of effective sample size,\n",
    "## and Rhat is the potential scale reduction factor on split chains (at \n",
    "## convergence, Rhat=1).\n",
    "```\n",
    "\n",
    "```r\n",
    "traceplot(sp_fit, pars = to_plot)\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/fit-sparse-model-1.png)<!-- -->\n",
    "\n",
    "### MCMC Efficiency comparison\n",
    " \n",
    "The main quantity of interest is the effective number of samples per unit time. \n",
    "Sparsity gives us an order of magnitude or so gains, mostly via reductions in run time. \n",
    "\n",
    "\n",
    "Model     Number of effective samples   Elapsed time (sec)   Effective samples / sec)\n",
    "-------  ----------------------------  -------------------  -------------------------\n",
    "full                         4485.084            488.56955                   9.180032\n",
    "sparse                       4418.415             38.52712                 114.683248\n",
    "\n",
    "### Posterior distribution comparison\n",
    "\n",
    "Let's compare the estimates to make sure that we get the same answer with both approaches. \n",
    "In this case, I've used more MCMC iterations than we would typically need in to get a better estimate of the tails of each marginal posterior distribution so that we can compare the 95% credible intervals among the two approaches. \n",
    "\n",
    "![](README_files/figure-html/compare-parameter-estimates-1.png)<!-- -->\n",
    "\n",
    "\n",
    "\n",
    "![](README_files/figure-html/unnamed-chunk-1-1.png)<!-- -->\n",
    "\n",
    "The two approaches give the same answers (more or less, with small differences arising due to MCMC sampling error). \n",
    "\n",
    "## Postscript: sparse IAR specification\n",
    "\n",
    "Although the IAR prior for $\\phi$ that results from $\\alpha = 1$ is improper, it remains popular (Besag, York, and Mollie, 1991). \n",
    "In practice, these models are typically fit with a sum to zero constraints: $\\sum_{i\\text{ in connected coponent}} \\phi_i = 0$ for each connected component of the graph. This allows us to interpret both the overall mean and the component-wise means.\n",
    "\n",
    "With $\\alpha$ fixed to one, we have: \n",
    "\n",
    "$$\\log(p(\\phi \\mid \\tau)) = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\text{det}^*(\\tau (D - W))) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "$$ = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\tau^{n-k} \\text{det}^*(D - W)) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "$$ = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\tau^{n-k}) + \\frac{1}{2} \\log(\\text{det}^*(D - W)) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "Here $\\text{det}^*(A)$ is the generalized determinant of the square matrix $A$ defined as the product of its non-zero eigenvalues, and $k$ is the number of connected components in the graph. For the Scottish Lip Cancer data, there is only one connected component and $k=1$.  The reason that we need to use the generalized determinant is that the precision matrix is, by definition, singular in intrinsic models as the support of the Gaussian distribution is on a subspace with fewer than $n$ dimensions.  For the classical ICAR(1) model, we know that the directions correpsonding to the zero eigenvalues are exactly the vectors that are constant on each connected component of the graph and hence $k$ is the number of connected components.\n",
    "\n",
    "\n",
    "Dropping additive constants, the quantity to increment becomes: \n",
    "\n",
    "$$ \\frac{1}{2} \\log(\\tau^{n-k}) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "And the corresponding Stan syntax would be:\n",
    "\n",
    "\n",
    "```\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper intrinsic autoregressive (IAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a IAR prior\n",
    "  * @param tau Precision parameter for the IAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of IAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_iar_lpdf(vector phi, real tau,\n",
    "    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {\n",
    "      row_vector[n] phit_D; // phi' * D\n",
    "      row_vector[n] phit_W; // phi' * W\n",
    "      vector[n] ldet_terms;\n",
    "    \n",
    "      phit_D = (phi .* D_sparse)';\n",
    "      phit_W = rep_row_vector(0, n);\n",
    "      for (i in 1:W_n) {\n",
    "        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "      }\n",
    "    \n",
    "      return 0.5 * ((n-1) * log(tau)\n",
    "                    - tau * (phit_D * phi - (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W; // adjacency matrix\n",
    "  int W_n;                // number of adjacent region pairs\n",
    "}\n",
    "transformed data {\n",
    "  int W_sparse[W_n, 2];   // adjacency pairs\n",
    "  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)\n",
    "  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  { // generate sparse representation for W\n",
    "  int counter;\n",
    "  counter = 1;\n",
    "  // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1:(n - 1)) {\n",
    "      for (j in (i + 1):n) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1:n) D_sparse[i] = sum(W[i]);\n",
    "  {\n",
    "    vector[n] invsqrtD;  \n",
    "    for (i in 1:n) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi_unscaled;\n",
    "  real<lower = 0> tau;\n",
    "}\n",
    "transformed parameters {\n",
    "  vector[n] phi; // brute force centering\n",
    "  phi = phi_unscaled - mean(phi_unscaled);\n",
    "}\n",
    "model {\n",
    "  phi_unscaled ~ sparse_iar(tau, W_sparse, D_sparse, lambda, n, W_n);\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "Besag, Julian, Jeremy York, and Annie Molli. \"Bayesian image restoration, with two applications in spatial statistics.\" Annals of the institute of statistical mathematics 43.1 (1991): 1-20.\n",
    "\n",
    "Gelfand, Alan E., and Penelope Vounatsou. \"Proper multivariate conditional autoregressive models for spatial data analysis.\" Biostatistics 4.1 (2003): 11-15.\n",
    "\n",
    "Jin, Xiaoping, Bradley P. Carlin, and Sudipto Banerjee. \"Generalized hierarchical multivariate CAR models for areal data.\" Biometrics 61.4 (2005): 950-961."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
