{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "\n",
    "from lib.car_model import CAR2\n",
    "from lib.utils import pad, new_name, create_matrices, get_digit_indices\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from theano import scan\n",
    "import theano.tensor as tt\n",
    "\n",
    "from pymc3.distributions import continuous\n",
    "from pymc3.distributions import distribution\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "d = x_train[12].copy()\n",
    "x_train = x_train/255\n",
    "x_test  = x_test/255\n",
    "\n",
    "def subset(label, data=x_train, labels=y_train):\n",
    "    '''\n",
    "    e.g. subset(3) -> gets all pictures of digit 3\n",
    "    '''\n",
    "    assert label >= 0 and label <= 9\n",
    "    \n",
    "    return data[np.argwhere(labels == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, figsize=(16, 7)):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image, cmap='Greys')\n",
    "    \n",
    "def plot_many():\n",
    "    for i in range(10):\n",
    "        get_digit_indices(y_train, i, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 6.25, 255.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = d.reshape(-1,) # type int\n",
    "[np.percentile(d, q) for q in [0, 25, 50, 75, 100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583\n",
      "0\n",
      "79\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum([pixel == 0 for pixel in d]))\n",
    "print(sum([pixel == 1 for pixel in d]))\n",
    "print(sum([pixel == 254 for pixel in d]))\n",
    "print(sum([pixel == 255 for pixel in d]))\n",
    "print(sum([pixel == 256 for pixel in d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 749,
       "width": 735
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy = x_train[12]\n",
    "plot(dummy, figsize=(20, 13))\n",
    "_ = plt.title('Label: ' + str(y_train[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dummy).to_csv('images/handwritten_digit.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.concatenate(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from https://docs.pymc.io/notebooks/PyMC3_tips_and_heuristic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "position_matrix = np.linspace(0, 28*28 - 1, num=28*28).astype(np.int64).reshape(28, 28)\n",
    "count = 0\n",
    "\n",
    "for i, row in enumerate(position_matrix):\n",
    "    for j, col in enumerate(position_matrix[i]):\n",
    "        assert position_matrix[i][j] == col\n",
    "        \n",
    "        temp = []\n",
    "\n",
    "        # change these loops if we do not want to\n",
    "        # include diagonal elements in adj matrix\n",
    "        for delta_i in [-1, 0, 1]:\n",
    "            for delta_j in [-1, 0, 1]:\n",
    "                if ((i + delta_i) // 28 == 0) and ((j + delta_j) // 28 == 0):    \n",
    "                    temp.append(position_matrix[i + delta_i][j + delta_j])\n",
    "        \n",
    "\n",
    "        temp.remove(col)\n",
    "        temp.sort()\n",
    "        adj.append(temp)\n",
    "        \n",
    "weights = [list(np.ones_like(adj_elems).astype(np.int64)) for adj_elems in adj]\n",
    "\n",
    "# below is taken from the pymc3 CAR tutorial website\n",
    "maxwz = max([sum(w) for w in weights])\n",
    "N = len(weights)\n",
    "wmat2 = np.zeros((N, N))\n",
    "amat2 = np.zeros((N, N), dtype='int32')\n",
    "for i, a in enumerate(adj):\n",
    "    amat2[i, a] = 1\n",
    "    wmat2[i, a] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 28, 29],\n",
       " [0, 2, 28, 29, 30],\n",
       " [1, 3, 29, 30, 31],\n",
       " [2, 4, 30, 31, 32],\n",
       " [3, 5, 31, 32, 33],\n",
       " [4, 6, 32, 33, 34],\n",
       " [5, 7, 33, 34, 35],\n",
       " [6, 8, 34, 35, 36],\n",
       " [7, 9, 35, 36, 37],\n",
       " [8, 10, 36, 37, 38],\n",
       " [9, 11, 37, 38, 39],\n",
       " [10, 12, 38, 39, 40],\n",
       " [11, 13, 39, 40, 41],\n",
       " [12, 14, 40, 41, 42],\n",
       " [13, 15, 41, 42, 43],\n",
       " [14, 16, 42, 43, 44],\n",
       " [15, 17, 43, 44, 45],\n",
       " [16, 18, 44, 45, 46],\n",
       " [17, 19, 45, 46, 47],\n",
       " [18, 20, 46, 47, 48],\n",
       " [19, 21, 47, 48, 49],\n",
       " [20, 22, 48, 49, 50],\n",
       " [21, 23, 49, 50, 51],\n",
       " [22, 24, 50, 51, 52],\n",
       " [23, 25, 51, 52, 53],\n",
       " [24, 26, 52, 53, 54],\n",
       " [25, 27, 53, 54, 55],\n",
       " [26, 54, 55],\n",
       " [0, 1, 29, 56, 57],\n",
       " [0, 1, 2, 28, 30, 56, 57, 58],\n",
       " [1, 2, 3, 29, 31, 57, 58, 59],\n",
       " [2, 3, 4, 30, 32, 58, 59, 60],\n",
       " [3, 4, 5, 31, 33, 59, 60, 61],\n",
       " [4, 5, 6, 32, 34, 60, 61, 62],\n",
       " [5, 6, 7, 33, 35, 61, 62, 63],\n",
       " [6, 7, 8, 34, 36, 62, 63, 64],\n",
       " [7, 8, 9, 35, 37, 63, 64, 65],\n",
       " [8, 9, 10, 36, 38, 64, 65, 66],\n",
       " [9, 10, 11, 37, 39, 65, 66, 67],\n",
       " [10, 11, 12, 38, 40, 66, 67, 68],\n",
       " [11, 12, 13, 39, 41, 67, 68, 69],\n",
       " [12, 13, 14, 40, 42, 68, 69, 70],\n",
       " [13, 14, 15, 41, 43, 69, 70, 71],\n",
       " [14, 15, 16, 42, 44, 70, 71, 72],\n",
       " [15, 16, 17, 43, 45, 71, 72, 73],\n",
       " [16, 17, 18, 44, 46, 72, 73, 74],\n",
       " [17, 18, 19, 45, 47, 73, 74, 75],\n",
       " [18, 19, 20, 46, 48, 74, 75, 76],\n",
       " [19, 20, 21, 47, 49, 75, 76, 77],\n",
       " [20, 21, 22, 48, 50, 76, 77, 78],\n",
       " [21, 22, 23, 49, 51, 77, 78, 79],\n",
       " [22, 23, 24, 50, 52, 78, 79, 80],\n",
       " [23, 24, 25, 51, 53, 79, 80, 81],\n",
       " [24, 25, 26, 52, 54, 80, 81, 82],\n",
       " [25, 26, 27, 53, 55, 81, 82, 83],\n",
       " [26, 27, 54, 82, 83],\n",
       " [28, 29, 57, 84, 85],\n",
       " [28, 29, 30, 56, 58, 84, 85, 86],\n",
       " [29, 30, 31, 57, 59, 85, 86, 87],\n",
       " [30, 31, 32, 58, 60, 86, 87, 88],\n",
       " [31, 32, 33, 59, 61, 87, 88, 89],\n",
       " [32, 33, 34, 60, 62, 88, 89, 90],\n",
       " [33, 34, 35, 61, 63, 89, 90, 91],\n",
       " [34, 35, 36, 62, 64, 90, 91, 92],\n",
       " [35, 36, 37, 63, 65, 91, 92, 93],\n",
       " [36, 37, 38, 64, 66, 92, 93, 94],\n",
       " [37, 38, 39, 65, 67, 93, 94, 95],\n",
       " [38, 39, 40, 66, 68, 94, 95, 96],\n",
       " [39, 40, 41, 67, 69, 95, 96, 97],\n",
       " [40, 41, 42, 68, 70, 96, 97, 98],\n",
       " [41, 42, 43, 69, 71, 97, 98, 99],\n",
       " [42, 43, 44, 70, 72, 98, 99, 100],\n",
       " [43, 44, 45, 71, 73, 99, 100, 101],\n",
       " [44, 45, 46, 72, 74, 100, 101, 102],\n",
       " [45, 46, 47, 73, 75, 101, 102, 103],\n",
       " [46, 47, 48, 74, 76, 102, 103, 104],\n",
       " [47, 48, 49, 75, 77, 103, 104, 105],\n",
       " [48, 49, 50, 76, 78, 104, 105, 106],\n",
       " [49, 50, 51, 77, 79, 105, 106, 107],\n",
       " [50, 51, 52, 78, 80, 106, 107, 108],\n",
       " [51, 52, 53, 79, 81, 107, 108, 109],\n",
       " [52, 53, 54, 80, 82, 108, 109, 110],\n",
       " [53, 54, 55, 81, 83, 109, 110, 111],\n",
       " [54, 55, 82, 110, 111],\n",
       " [56, 57, 85, 112, 113],\n",
       " [56, 57, 58, 84, 86, 112, 113, 114],\n",
       " [57, 58, 59, 85, 87, 113, 114, 115],\n",
       " [58, 59, 60, 86, 88, 114, 115, 116],\n",
       " [59, 60, 61, 87, 89, 115, 116, 117],\n",
       " [60, 61, 62, 88, 90, 116, 117, 118],\n",
       " [61, 62, 63, 89, 91, 117, 118, 119],\n",
       " [62, 63, 64, 90, 92, 118, 119, 120],\n",
       " [63, 64, 65, 91, 93, 119, 120, 121],\n",
       " [64, 65, 66, 92, 94, 120, 121, 122],\n",
       " [65, 66, 67, 93, 95, 121, 122, 123],\n",
       " [66, 67, 68, 94, 96, 122, 123, 124],\n",
       " [67, 68, 69, 95, 97, 123, 124, 125],\n",
       " [68, 69, 70, 96, 98, 124, 125, 126],\n",
       " [69, 70, 71, 97, 99, 125, 126, 127],\n",
       " [70, 71, 72, 98, 100, 126, 127, 128],\n",
       " [71, 72, 73, 99, 101, 127, 128, 129],\n",
       " [72, 73, 74, 100, 102, 128, 129, 130],\n",
       " [73, 74, 75, 101, 103, 129, 130, 131],\n",
       " [74, 75, 76, 102, 104, 130, 131, 132],\n",
       " [75, 76, 77, 103, 105, 131, 132, 133],\n",
       " [76, 77, 78, 104, 106, 132, 133, 134],\n",
       " [77, 78, 79, 105, 107, 133, 134, 135],\n",
       " [78, 79, 80, 106, 108, 134, 135, 136],\n",
       " [79, 80, 81, 107, 109, 135, 136, 137],\n",
       " [80, 81, 82, 108, 110, 136, 137, 138],\n",
       " [81, 82, 83, 109, 111, 137, 138, 139],\n",
       " [82, 83, 110, 138, 139],\n",
       " [84, 85, 113, 140, 141],\n",
       " [84, 85, 86, 112, 114, 140, 141, 142],\n",
       " [85, 86, 87, 113, 115, 141, 142, 143],\n",
       " [86, 87, 88, 114, 116, 142, 143, 144],\n",
       " [87, 88, 89, 115, 117, 143, 144, 145],\n",
       " [88, 89, 90, 116, 118, 144, 145, 146],\n",
       " [89, 90, 91, 117, 119, 145, 146, 147],\n",
       " [90, 91, 92, 118, 120, 146, 147, 148],\n",
       " [91, 92, 93, 119, 121, 147, 148, 149],\n",
       " [92, 93, 94, 120, 122, 148, 149, 150],\n",
       " [93, 94, 95, 121, 123, 149, 150, 151],\n",
       " [94, 95, 96, 122, 124, 150, 151, 152],\n",
       " [95, 96, 97, 123, 125, 151, 152, 153],\n",
       " [96, 97, 98, 124, 126, 152, 153, 154],\n",
       " [97, 98, 99, 125, 127, 153, 154, 155],\n",
       " [98, 99, 100, 126, 128, 154, 155, 156],\n",
       " [99, 100, 101, 127, 129, 155, 156, 157],\n",
       " [100, 101, 102, 128, 130, 156, 157, 158],\n",
       " [101, 102, 103, 129, 131, 157, 158, 159],\n",
       " [102, 103, 104, 130, 132, 158, 159, 160],\n",
       " [103, 104, 105, 131, 133, 159, 160, 161],\n",
       " [104, 105, 106, 132, 134, 160, 161, 162],\n",
       " [105, 106, 107, 133, 135, 161, 162, 163],\n",
       " [106, 107, 108, 134, 136, 162, 163, 164],\n",
       " [107, 108, 109, 135, 137, 163, 164, 165],\n",
       " [108, 109, 110, 136, 138, 164, 165, 166],\n",
       " [109, 110, 111, 137, 139, 165, 166, 167],\n",
       " [110, 111, 138, 166, 167],\n",
       " [112, 113, 141, 168, 169],\n",
       " [112, 113, 114, 140, 142, 168, 169, 170],\n",
       " [113, 114, 115, 141, 143, 169, 170, 171],\n",
       " [114, 115, 116, 142, 144, 170, 171, 172],\n",
       " [115, 116, 117, 143, 145, 171, 172, 173],\n",
       " [116, 117, 118, 144, 146, 172, 173, 174],\n",
       " [117, 118, 119, 145, 147, 173, 174, 175],\n",
       " [118, 119, 120, 146, 148, 174, 175, 176],\n",
       " [119, 120, 121, 147, 149, 175, 176, 177],\n",
       " [120, 121, 122, 148, 150, 176, 177, 178],\n",
       " [121, 122, 123, 149, 151, 177, 178, 179],\n",
       " [122, 123, 124, 150, 152, 178, 179, 180],\n",
       " [123, 124, 125, 151, 153, 179, 180, 181],\n",
       " [124, 125, 126, 152, 154, 180, 181, 182],\n",
       " [125, 126, 127, 153, 155, 181, 182, 183],\n",
       " [126, 127, 128, 154, 156, 182, 183, 184],\n",
       " [127, 128, 129, 155, 157, 183, 184, 185],\n",
       " [128, 129, 130, 156, 158, 184, 185, 186],\n",
       " [129, 130, 131, 157, 159, 185, 186, 187],\n",
       " [130, 131, 132, 158, 160, 186, 187, 188],\n",
       " [131, 132, 133, 159, 161, 187, 188, 189],\n",
       " [132, 133, 134, 160, 162, 188, 189, 190],\n",
       " [133, 134, 135, 161, 163, 189, 190, 191],\n",
       " [134, 135, 136, 162, 164, 190, 191, 192],\n",
       " [135, 136, 137, 163, 165, 191, 192, 193],\n",
       " [136, 137, 138, 164, 166, 192, 193, 194],\n",
       " [137, 138, 139, 165, 167, 193, 194, 195],\n",
       " [138, 139, 166, 194, 195],\n",
       " [140, 141, 169, 196, 197],\n",
       " [140, 141, 142, 168, 170, 196, 197, 198],\n",
       " [141, 142, 143, 169, 171, 197, 198, 199],\n",
       " [142, 143, 144, 170, 172, 198, 199, 200],\n",
       " [143, 144, 145, 171, 173, 199, 200, 201],\n",
       " [144, 145, 146, 172, 174, 200, 201, 202],\n",
       " [145, 146, 147, 173, 175, 201, 202, 203],\n",
       " [146, 147, 148, 174, 176, 202, 203, 204],\n",
       " [147, 148, 149, 175, 177, 203, 204, 205],\n",
       " [148, 149, 150, 176, 178, 204, 205, 206],\n",
       " [149, 150, 151, 177, 179, 205, 206, 207],\n",
       " [150, 151, 152, 178, 180, 206, 207, 208],\n",
       " [151, 152, 153, 179, 181, 207, 208, 209],\n",
       " [152, 153, 154, 180, 182, 208, 209, 210],\n",
       " [153, 154, 155, 181, 183, 209, 210, 211],\n",
       " [154, 155, 156, 182, 184, 210, 211, 212],\n",
       " [155, 156, 157, 183, 185, 211, 212, 213],\n",
       " [156, 157, 158, 184, 186, 212, 213, 214],\n",
       " [157, 158, 159, 185, 187, 213, 214, 215],\n",
       " [158, 159, 160, 186, 188, 214, 215, 216],\n",
       " [159, 160, 161, 187, 189, 215, 216, 217],\n",
       " [160, 161, 162, 188, 190, 216, 217, 218],\n",
       " [161, 162, 163, 189, 191, 217, 218, 219],\n",
       " [162, 163, 164, 190, 192, 218, 219, 220],\n",
       " [163, 164, 165, 191, 193, 219, 220, 221],\n",
       " [164, 165, 166, 192, 194, 220, 221, 222],\n",
       " [165, 166, 167, 193, 195, 221, 222, 223],\n",
       " [166, 167, 194, 222, 223],\n",
       " [168, 169, 197, 224, 225],\n",
       " [168, 169, 170, 196, 198, 224, 225, 226],\n",
       " [169, 170, 171, 197, 199, 225, 226, 227],\n",
       " [170, 171, 172, 198, 200, 226, 227, 228],\n",
       " [171, 172, 173, 199, 201, 227, 228, 229],\n",
       " [172, 173, 174, 200, 202, 228, 229, 230],\n",
       " [173, 174, 175, 201, 203, 229, 230, 231],\n",
       " [174, 175, 176, 202, 204, 230, 231, 232],\n",
       " [175, 176, 177, 203, 205, 231, 232, 233],\n",
       " [176, 177, 178, 204, 206, 232, 233, 234],\n",
       " [177, 178, 179, 205, 207, 233, 234, 235],\n",
       " [178, 179, 180, 206, 208, 234, 235, 236],\n",
       " [179, 180, 181, 207, 209, 235, 236, 237],\n",
       " [180, 181, 182, 208, 210, 236, 237, 238],\n",
       " [181, 182, 183, 209, 211, 237, 238, 239],\n",
       " [182, 183, 184, 210, 212, 238, 239, 240],\n",
       " [183, 184, 185, 211, 213, 239, 240, 241],\n",
       " [184, 185, 186, 212, 214, 240, 241, 242],\n",
       " [185, 186, 187, 213, 215, 241, 242, 243],\n",
       " [186, 187, 188, 214, 216, 242, 243, 244],\n",
       " [187, 188, 189, 215, 217, 243, 244, 245],\n",
       " [188, 189, 190, 216, 218, 244, 245, 246],\n",
       " [189, 190, 191, 217, 219, 245, 246, 247],\n",
       " [190, 191, 192, 218, 220, 246, 247, 248],\n",
       " [191, 192, 193, 219, 221, 247, 248, 249],\n",
       " [192, 193, 194, 220, 222, 248, 249, 250],\n",
       " [193, 194, 195, 221, 223, 249, 250, 251],\n",
       " [194, 195, 222, 250, 251],\n",
       " [196, 197, 225, 252, 253],\n",
       " [196, 197, 198, 224, 226, 252, 253, 254],\n",
       " [197, 198, 199, 225, 227, 253, 254, 255],\n",
       " [198, 199, 200, 226, 228, 254, 255, 256],\n",
       " [199, 200, 201, 227, 229, 255, 256, 257],\n",
       " [200, 201, 202, 228, 230, 256, 257, 258],\n",
       " [201, 202, 203, 229, 231, 257, 258, 259],\n",
       " [202, 203, 204, 230, 232, 258, 259, 260],\n",
       " [203, 204, 205, 231, 233, 259, 260, 261],\n",
       " [204, 205, 206, 232, 234, 260, 261, 262],\n",
       " [205, 206, 207, 233, 235, 261, 262, 263],\n",
       " [206, 207, 208, 234, 236, 262, 263, 264],\n",
       " [207, 208, 209, 235, 237, 263, 264, 265],\n",
       " [208, 209, 210, 236, 238, 264, 265, 266],\n",
       " [209, 210, 211, 237, 239, 265, 266, 267],\n",
       " [210, 211, 212, 238, 240, 266, 267, 268],\n",
       " [211, 212, 213, 239, 241, 267, 268, 269],\n",
       " [212, 213, 214, 240, 242, 268, 269, 270],\n",
       " [213, 214, 215, 241, 243, 269, 270, 271],\n",
       " [214, 215, 216, 242, 244, 270, 271, 272],\n",
       " [215, 216, 217, 243, 245, 271, 272, 273],\n",
       " [216, 217, 218, 244, 246, 272, 273, 274],\n",
       " [217, 218, 219, 245, 247, 273, 274, 275],\n",
       " [218, 219, 220, 246, 248, 274, 275, 276],\n",
       " [219, 220, 221, 247, 249, 275, 276, 277],\n",
       " [220, 221, 222, 248, 250, 276, 277, 278],\n",
       " [221, 222, 223, 249, 251, 277, 278, 279],\n",
       " [222, 223, 250, 278, 279],\n",
       " [224, 225, 253, 280, 281],\n",
       " [224, 225, 226, 252, 254, 280, 281, 282],\n",
       " [225, 226, 227, 253, 255, 281, 282, 283],\n",
       " [226, 227, 228, 254, 256, 282, 283, 284],\n",
       " [227, 228, 229, 255, 257, 283, 284, 285],\n",
       " [228, 229, 230, 256, 258, 284, 285, 286],\n",
       " [229, 230, 231, 257, 259, 285, 286, 287],\n",
       " [230, 231, 232, 258, 260, 286, 287, 288],\n",
       " [231, 232, 233, 259, 261, 287, 288, 289],\n",
       " [232, 233, 234, 260, 262, 288, 289, 290],\n",
       " [233, 234, 235, 261, 263, 289, 290, 291],\n",
       " [234, 235, 236, 262, 264, 290, 291, 292],\n",
       " [235, 236, 237, 263, 265, 291, 292, 293],\n",
       " [236, 237, 238, 264, 266, 292, 293, 294],\n",
       " [237, 238, 239, 265, 267, 293, 294, 295],\n",
       " [238, 239, 240, 266, 268, 294, 295, 296],\n",
       " [239, 240, 241, 267, 269, 295, 296, 297],\n",
       " [240, 241, 242, 268, 270, 296, 297, 298],\n",
       " [241, 242, 243, 269, 271, 297, 298, 299],\n",
       " [242, 243, 244, 270, 272, 298, 299, 300],\n",
       " [243, 244, 245, 271, 273, 299, 300, 301],\n",
       " [244, 245, 246, 272, 274, 300, 301, 302],\n",
       " [245, 246, 247, 273, 275, 301, 302, 303],\n",
       " [246, 247, 248, 274, 276, 302, 303, 304],\n",
       " [247, 248, 249, 275, 277, 303, 304, 305],\n",
       " [248, 249, 250, 276, 278, 304, 305, 306],\n",
       " [249, 250, 251, 277, 279, 305, 306, 307],\n",
       " [250, 251, 278, 306, 307],\n",
       " [252, 253, 281, 308, 309],\n",
       " [252, 253, 254, 280, 282, 308, 309, 310],\n",
       " [253, 254, 255, 281, 283, 309, 310, 311],\n",
       " [254, 255, 256, 282, 284, 310, 311, 312],\n",
       " [255, 256, 257, 283, 285, 311, 312, 313],\n",
       " [256, 257, 258, 284, 286, 312, 313, 314],\n",
       " [257, 258, 259, 285, 287, 313, 314, 315],\n",
       " [258, 259, 260, 286, 288, 314, 315, 316],\n",
       " [259, 260, 261, 287, 289, 315, 316, 317],\n",
       " [260, 261, 262, 288, 290, 316, 317, 318],\n",
       " [261, 262, 263, 289, 291, 317, 318, 319],\n",
       " [262, 263, 264, 290, 292, 318, 319, 320],\n",
       " [263, 264, 265, 291, 293, 319, 320, 321],\n",
       " [264, 265, 266, 292, 294, 320, 321, 322],\n",
       " [265, 266, 267, 293, 295, 321, 322, 323],\n",
       " [266, 267, 268, 294, 296, 322, 323, 324],\n",
       " [267, 268, 269, 295, 297, 323, 324, 325],\n",
       " [268, 269, 270, 296, 298, 324, 325, 326],\n",
       " [269, 270, 271, 297, 299, 325, 326, 327],\n",
       " [270, 271, 272, 298, 300, 326, 327, 328],\n",
       " [271, 272, 273, 299, 301, 327, 328, 329],\n",
       " [272, 273, 274, 300, 302, 328, 329, 330],\n",
       " [273, 274, 275, 301, 303, 329, 330, 331],\n",
       " [274, 275, 276, 302, 304, 330, 331, 332],\n",
       " [275, 276, 277, 303, 305, 331, 332, 333],\n",
       " [276, 277, 278, 304, 306, 332, 333, 334],\n",
       " [277, 278, 279, 305, 307, 333, 334, 335],\n",
       " [278, 279, 306, 334, 335],\n",
       " [280, 281, 309, 336, 337],\n",
       " [280, 281, 282, 308, 310, 336, 337, 338],\n",
       " [281, 282, 283, 309, 311, 337, 338, 339],\n",
       " [282, 283, 284, 310, 312, 338, 339, 340],\n",
       " [283, 284, 285, 311, 313, 339, 340, 341],\n",
       " [284, 285, 286, 312, 314, 340, 341, 342],\n",
       " [285, 286, 287, 313, 315, 341, 342, 343],\n",
       " [286, 287, 288, 314, 316, 342, 343, 344],\n",
       " [287, 288, 289, 315, 317, 343, 344, 345],\n",
       " [288, 289, 290, 316, 318, 344, 345, 346],\n",
       " [289, 290, 291, 317, 319, 345, 346, 347],\n",
       " [290, 291, 292, 318, 320, 346, 347, 348],\n",
       " [291, 292, 293, 319, 321, 347, 348, 349],\n",
       " [292, 293, 294, 320, 322, 348, 349, 350],\n",
       " [293, 294, 295, 321, 323, 349, 350, 351],\n",
       " [294, 295, 296, 322, 324, 350, 351, 352],\n",
       " [295, 296, 297, 323, 325, 351, 352, 353],\n",
       " [296, 297, 298, 324, 326, 352, 353, 354],\n",
       " [297, 298, 299, 325, 327, 353, 354, 355],\n",
       " [298, 299, 300, 326, 328, 354, 355, 356],\n",
       " [299, 300, 301, 327, 329, 355, 356, 357],\n",
       " [300, 301, 302, 328, 330, 356, 357, 358],\n",
       " [301, 302, 303, 329, 331, 357, 358, 359],\n",
       " [302, 303, 304, 330, 332, 358, 359, 360],\n",
       " [303, 304, 305, 331, 333, 359, 360, 361],\n",
       " [304, 305, 306, 332, 334, 360, 361, 362],\n",
       " [305, 306, 307, 333, 335, 361, 362, 363],\n",
       " [306, 307, 334, 362, 363],\n",
       " [308, 309, 337, 364, 365],\n",
       " [308, 309, 310, 336, 338, 364, 365, 366],\n",
       " [309, 310, 311, 337, 339, 365, 366, 367],\n",
       " [310, 311, 312, 338, 340, 366, 367, 368],\n",
       " [311, 312, 313, 339, 341, 367, 368, 369],\n",
       " [312, 313, 314, 340, 342, 368, 369, 370],\n",
       " [313, 314, 315, 341, 343, 369, 370, 371],\n",
       " [314, 315, 316, 342, 344, 370, 371, 372],\n",
       " [315, 316, 317, 343, 345, 371, 372, 373],\n",
       " [316, 317, 318, 344, 346, 372, 373, 374],\n",
       " [317, 318, 319, 345, 347, 373, 374, 375],\n",
       " [318, 319, 320, 346, 348, 374, 375, 376],\n",
       " [319, 320, 321, 347, 349, 375, 376, 377],\n",
       " [320, 321, 322, 348, 350, 376, 377, 378],\n",
       " [321, 322, 323, 349, 351, 377, 378, 379],\n",
       " [322, 323, 324, 350, 352, 378, 379, 380],\n",
       " [323, 324, 325, 351, 353, 379, 380, 381],\n",
       " [324, 325, 326, 352, 354, 380, 381, 382],\n",
       " [325, 326, 327, 353, 355, 381, 382, 383],\n",
       " [326, 327, 328, 354, 356, 382, 383, 384],\n",
       " [327, 328, 329, 355, 357, 383, 384, 385],\n",
       " [328, 329, 330, 356, 358, 384, 385, 386],\n",
       " [329, 330, 331, 357, 359, 385, 386, 387],\n",
       " [330, 331, 332, 358, 360, 386, 387, 388],\n",
       " [331, 332, 333, 359, 361, 387, 388, 389],\n",
       " [332, 333, 334, 360, 362, 388, 389, 390],\n",
       " [333, 334, 335, 361, 363, 389, 390, 391],\n",
       " [334, 335, 362, 390, 391],\n",
       " [336, 337, 365, 392, 393],\n",
       " [336, 337, 338, 364, 366, 392, 393, 394],\n",
       " [337, 338, 339, 365, 367, 393, 394, 395],\n",
       " [338, 339, 340, 366, 368, 394, 395, 396],\n",
       " [339, 340, 341, 367, 369, 395, 396, 397],\n",
       " [340, 341, 342, 368, 370, 396, 397, 398],\n",
       " [341, 342, 343, 369, 371, 397, 398, 399],\n",
       " [342, 343, 344, 370, 372, 398, 399, 400],\n",
       " [343, 344, 345, 371, 373, 399, 400, 401],\n",
       " [344, 345, 346, 372, 374, 400, 401, 402],\n",
       " [345, 346, 347, 373, 375, 401, 402, 403],\n",
       " [346, 347, 348, 374, 376, 402, 403, 404],\n",
       " [347, 348, 349, 375, 377, 403, 404, 405],\n",
       " [348, 349, 350, 376, 378, 404, 405, 406],\n",
       " [349, 350, 351, 377, 379, 405, 406, 407],\n",
       " [350, 351, 352, 378, 380, 406, 407, 408],\n",
       " [351, 352, 353, 379, 381, 407, 408, 409],\n",
       " [352, 353, 354, 380, 382, 408, 409, 410],\n",
       " [353, 354, 355, 381, 383, 409, 410, 411],\n",
       " [354, 355, 356, 382, 384, 410, 411, 412],\n",
       " [355, 356, 357, 383, 385, 411, 412, 413],\n",
       " [356, 357, 358, 384, 386, 412, 413, 414],\n",
       " [357, 358, 359, 385, 387, 413, 414, 415],\n",
       " [358, 359, 360, 386, 388, 414, 415, 416],\n",
       " [359, 360, 361, 387, 389, 415, 416, 417],\n",
       " [360, 361, 362, 388, 390, 416, 417, 418],\n",
       " [361, 362, 363, 389, 391, 417, 418, 419],\n",
       " [362, 363, 390, 418, 419],\n",
       " [364, 365, 393, 420, 421],\n",
       " [364, 365, 366, 392, 394, 420, 421, 422],\n",
       " [365, 366, 367, 393, 395, 421, 422, 423],\n",
       " [366, 367, 368, 394, 396, 422, 423, 424],\n",
       " [367, 368, 369, 395, 397, 423, 424, 425],\n",
       " [368, 369, 370, 396, 398, 424, 425, 426],\n",
       " [369, 370, 371, 397, 399, 425, 426, 427],\n",
       " [370, 371, 372, 398, 400, 426, 427, 428],\n",
       " [371, 372, 373, 399, 401, 427, 428, 429],\n",
       " [372, 373, 374, 400, 402, 428, 429, 430],\n",
       " [373, 374, 375, 401, 403, 429, 430, 431],\n",
       " [374, 375, 376, 402, 404, 430, 431, 432],\n",
       " [375, 376, 377, 403, 405, 431, 432, 433],\n",
       " [376, 377, 378, 404, 406, 432, 433, 434],\n",
       " [377, 378, 379, 405, 407, 433, 434, 435],\n",
       " [378, 379, 380, 406, 408, 434, 435, 436],\n",
       " [379, 380, 381, 407, 409, 435, 436, 437],\n",
       " [380, 381, 382, 408, 410, 436, 437, 438],\n",
       " [381, 382, 383, 409, 411, 437, 438, 439],\n",
       " [382, 383, 384, 410, 412, 438, 439, 440],\n",
       " [383, 384, 385, 411, 413, 439, 440, 441],\n",
       " [384, 385, 386, 412, 414, 440, 441, 442],\n",
       " [385, 386, 387, 413, 415, 441, 442, 443],\n",
       " [386, 387, 388, 414, 416, 442, 443, 444],\n",
       " [387, 388, 389, 415, 417, 443, 444, 445],\n",
       " [388, 389, 390, 416, 418, 444, 445, 446],\n",
       " [389, 390, 391, 417, 419, 445, 446, 447],\n",
       " [390, 391, 418, 446, 447],\n",
       " [392, 393, 421, 448, 449],\n",
       " [392, 393, 394, 420, 422, 448, 449, 450],\n",
       " [393, 394, 395, 421, 423, 449, 450, 451],\n",
       " [394, 395, 396, 422, 424, 450, 451, 452],\n",
       " [395, 396, 397, 423, 425, 451, 452, 453],\n",
       " [396, 397, 398, 424, 426, 452, 453, 454],\n",
       " [397, 398, 399, 425, 427, 453, 454, 455],\n",
       " [398, 399, 400, 426, 428, 454, 455, 456],\n",
       " [399, 400, 401, 427, 429, 455, 456, 457],\n",
       " [400, 401, 402, 428, 430, 456, 457, 458],\n",
       " [401, 402, 403, 429, 431, 457, 458, 459],\n",
       " [402, 403, 404, 430, 432, 458, 459, 460],\n",
       " [403, 404, 405, 431, 433, 459, 460, 461],\n",
       " [404, 405, 406, 432, 434, 460, 461, 462],\n",
       " [405, 406, 407, 433, 435, 461, 462, 463],\n",
       " [406, 407, 408, 434, 436, 462, 463, 464],\n",
       " [407, 408, 409, 435, 437, 463, 464, 465],\n",
       " [408, 409, 410, 436, 438, 464, 465, 466],\n",
       " [409, 410, 411, 437, 439, 465, 466, 467],\n",
       " [410, 411, 412, 438, 440, 466, 467, 468],\n",
       " [411, 412, 413, 439, 441, 467, 468, 469],\n",
       " [412, 413, 414, 440, 442, 468, 469, 470],\n",
       " [413, 414, 415, 441, 443, 469, 470, 471],\n",
       " [414, 415, 416, 442, 444, 470, 471, 472],\n",
       " [415, 416, 417, 443, 445, 471, 472, 473],\n",
       " [416, 417, 418, 444, 446, 472, 473, 474],\n",
       " [417, 418, 419, 445, 447, 473, 474, 475],\n",
       " [418, 419, 446, 474, 475],\n",
       " [420, 421, 449, 476, 477],\n",
       " [420, 421, 422, 448, 450, 476, 477, 478],\n",
       " [421, 422, 423, 449, 451, 477, 478, 479],\n",
       " [422, 423, 424, 450, 452, 478, 479, 480],\n",
       " [423, 424, 425, 451, 453, 479, 480, 481],\n",
       " [424, 425, 426, 452, 454, 480, 481, 482],\n",
       " [425, 426, 427, 453, 455, 481, 482, 483],\n",
       " [426, 427, 428, 454, 456, 482, 483, 484],\n",
       " [427, 428, 429, 455, 457, 483, 484, 485],\n",
       " [428, 429, 430, 456, 458, 484, 485, 486],\n",
       " [429, 430, 431, 457, 459, 485, 486, 487],\n",
       " [430, 431, 432, 458, 460, 486, 487, 488],\n",
       " [431, 432, 433, 459, 461, 487, 488, 489],\n",
       " [432, 433, 434, 460, 462, 488, 489, 490],\n",
       " [433, 434, 435, 461, 463, 489, 490, 491],\n",
       " [434, 435, 436, 462, 464, 490, 491, 492],\n",
       " [435, 436, 437, 463, 465, 491, 492, 493],\n",
       " [436, 437, 438, 464, 466, 492, 493, 494],\n",
       " [437, 438, 439, 465, 467, 493, 494, 495],\n",
       " [438, 439, 440, 466, 468, 494, 495, 496],\n",
       " [439, 440, 441, 467, 469, 495, 496, 497],\n",
       " [440, 441, 442, 468, 470, 496, 497, 498],\n",
       " [441, 442, 443, 469, 471, 497, 498, 499],\n",
       " [442, 443, 444, 470, 472, 498, 499, 500],\n",
       " [443, 444, 445, 471, 473, 499, 500, 501],\n",
       " [444, 445, 446, 472, 474, 500, 501, 502],\n",
       " [445, 446, 447, 473, 475, 501, 502, 503],\n",
       " [446, 447, 474, 502, 503],\n",
       " [448, 449, 477, 504, 505],\n",
       " [448, 449, 450, 476, 478, 504, 505, 506],\n",
       " [449, 450, 451, 477, 479, 505, 506, 507],\n",
       " [450, 451, 452, 478, 480, 506, 507, 508],\n",
       " [451, 452, 453, 479, 481, 507, 508, 509],\n",
       " [452, 453, 454, 480, 482, 508, 509, 510],\n",
       " [453, 454, 455, 481, 483, 509, 510, 511],\n",
       " [454, 455, 456, 482, 484, 510, 511, 512],\n",
       " [455, 456, 457, 483, 485, 511, 512, 513],\n",
       " [456, 457, 458, 484, 486, 512, 513, 514],\n",
       " [457, 458, 459, 485, 487, 513, 514, 515],\n",
       " [458, 459, 460, 486, 488, 514, 515, 516],\n",
       " [459, 460, 461, 487, 489, 515, 516, 517],\n",
       " [460, 461, 462, 488, 490, 516, 517, 518],\n",
       " [461, 462, 463, 489, 491, 517, 518, 519],\n",
       " [462, 463, 464, 490, 492, 518, 519, 520],\n",
       " [463, 464, 465, 491, 493, 519, 520, 521],\n",
       " [464, 465, 466, 492, 494, 520, 521, 522],\n",
       " [465, 466, 467, 493, 495, 521, 522, 523],\n",
       " [466, 467, 468, 494, 496, 522, 523, 524],\n",
       " [467, 468, 469, 495, 497, 523, 524, 525],\n",
       " [468, 469, 470, 496, 498, 524, 525, 526],\n",
       " [469, 470, 471, 497, 499, 525, 526, 527],\n",
       " [470, 471, 472, 498, 500, 526, 527, 528],\n",
       " [471, 472, 473, 499, 501, 527, 528, 529],\n",
       " [472, 473, 474, 500, 502, 528, 529, 530],\n",
       " [473, 474, 475, 501, 503, 529, 530, 531],\n",
       " [474, 475, 502, 530, 531],\n",
       " [476, 477, 505, 532, 533],\n",
       " [476, 477, 478, 504, 506, 532, 533, 534],\n",
       " [477, 478, 479, 505, 507, 533, 534, 535],\n",
       " [478, 479, 480, 506, 508, 534, 535, 536],\n",
       " [479, 480, 481, 507, 509, 535, 536, 537],\n",
       " [480, 481, 482, 508, 510, 536, 537, 538],\n",
       " [481, 482, 483, 509, 511, 537, 538, 539],\n",
       " [482, 483, 484, 510, 512, 538, 539, 540],\n",
       " [483, 484, 485, 511, 513, 539, 540, 541],\n",
       " [484, 485, 486, 512, 514, 540, 541, 542],\n",
       " [485, 486, 487, 513, 515, 541, 542, 543],\n",
       " [486, 487, 488, 514, 516, 542, 543, 544],\n",
       " [487, 488, 489, 515, 517, 543, 544, 545],\n",
       " [488, 489, 490, 516, 518, 544, 545, 546],\n",
       " [489, 490, 491, 517, 519, 545, 546, 547],\n",
       " [490, 491, 492, 518, 520, 546, 547, 548],\n",
       " [491, 492, 493, 519, 521, 547, 548, 549],\n",
       " [492, 493, 494, 520, 522, 548, 549, 550],\n",
       " [493, 494, 495, 521, 523, 549, 550, 551],\n",
       " [494, 495, 496, 522, 524, 550, 551, 552],\n",
       " [495, 496, 497, 523, 525, 551, 552, 553],\n",
       " [496, 497, 498, 524, 526, 552, 553, 554],\n",
       " [497, 498, 499, 525, 527, 553, 554, 555],\n",
       " [498, 499, 500, 526, 528, 554, 555, 556],\n",
       " [499, 500, 501, 527, 529, 555, 556, 557],\n",
       " [500, 501, 502, 528, 530, 556, 557, 558],\n",
       " [501, 502, 503, 529, 531, 557, 558, 559],\n",
       " [502, 503, 530, 558, 559],\n",
       " [504, 505, 533, 560, 561],\n",
       " [504, 505, 506, 532, 534, 560, 561, 562],\n",
       " [505, 506, 507, 533, 535, 561, 562, 563],\n",
       " [506, 507, 508, 534, 536, 562, 563, 564],\n",
       " [507, 508, 509, 535, 537, 563, 564, 565],\n",
       " [508, 509, 510, 536, 538, 564, 565, 566],\n",
       " [509, 510, 511, 537, 539, 565, 566, 567],\n",
       " [510, 511, 512, 538, 540, 566, 567, 568],\n",
       " [511, 512, 513, 539, 541, 567, 568, 569],\n",
       " [512, 513, 514, 540, 542, 568, 569, 570],\n",
       " [513, 514, 515, 541, 543, 569, 570, 571],\n",
       " [514, 515, 516, 542, 544, 570, 571, 572],\n",
       " [515, 516, 517, 543, 545, 571, 572, 573],\n",
       " [516, 517, 518, 544, 546, 572, 573, 574],\n",
       " [517, 518, 519, 545, 547, 573, 574, 575],\n",
       " [518, 519, 520, 546, 548, 574, 575, 576],\n",
       " [519, 520, 521, 547, 549, 575, 576, 577],\n",
       " [520, 521, 522, 548, 550, 576, 577, 578],\n",
       " [521, 522, 523, 549, 551, 577, 578, 579],\n",
       " [522, 523, 524, 550, 552, 578, 579, 580],\n",
       " [523, 524, 525, 551, 553, 579, 580, 581],\n",
       " [524, 525, 526, 552, 554, 580, 581, 582],\n",
       " [525, 526, 527, 553, 555, 581, 582, 583],\n",
       " [526, 527, 528, 554, 556, 582, 583, 584],\n",
       " [527, 528, 529, 555, 557, 583, 584, 585],\n",
       " [528, 529, 530, 556, 558, 584, 585, 586],\n",
       " [529, 530, 531, 557, 559, 585, 586, 587],\n",
       " [530, 531, 558, 586, 587],\n",
       " [532, 533, 561, 588, 589],\n",
       " [532, 533, 534, 560, 562, 588, 589, 590],\n",
       " [533, 534, 535, 561, 563, 589, 590, 591],\n",
       " [534, 535, 536, 562, 564, 590, 591, 592],\n",
       " [535, 536, 537, 563, 565, 591, 592, 593],\n",
       " [536, 537, 538, 564, 566, 592, 593, 594],\n",
       " [537, 538, 539, 565, 567, 593, 594, 595],\n",
       " [538, 539, 540, 566, 568, 594, 595, 596],\n",
       " [539, 540, 541, 567, 569, 595, 596, 597],\n",
       " [540, 541, 542, 568, 570, 596, 597, 598],\n",
       " [541, 542, 543, 569, 571, 597, 598, 599],\n",
       " [542, 543, 544, 570, 572, 598, 599, 600],\n",
       " [543, 544, 545, 571, 573, 599, 600, 601],\n",
       " [544, 545, 546, 572, 574, 600, 601, 602],\n",
       " [545, 546, 547, 573, 575, 601, 602, 603],\n",
       " [546, 547, 548, 574, 576, 602, 603, 604],\n",
       " [547, 548, 549, 575, 577, 603, 604, 605],\n",
       " [548, 549, 550, 576, 578, 604, 605, 606],\n",
       " [549, 550, 551, 577, 579, 605, 606, 607],\n",
       " [550, 551, 552, 578, 580, 606, 607, 608],\n",
       " [551, 552, 553, 579, 581, 607, 608, 609],\n",
       " [552, 553, 554, 580, 582, 608, 609, 610],\n",
       " [553, 554, 555, 581, 583, 609, 610, 611],\n",
       " [554, 555, 556, 582, 584, 610, 611, 612],\n",
       " [555, 556, 557, 583, 585, 611, 612, 613],\n",
       " [556, 557, 558, 584, 586, 612, 613, 614],\n",
       " [557, 558, 559, 585, 587, 613, 614, 615],\n",
       " [558, 559, 586, 614, 615],\n",
       " [560, 561, 589, 616, 617],\n",
       " [560, 561, 562, 588, 590, 616, 617, 618],\n",
       " [561, 562, 563, 589, 591, 617, 618, 619],\n",
       " [562, 563, 564, 590, 592, 618, 619, 620],\n",
       " [563, 564, 565, 591, 593, 619, 620, 621],\n",
       " [564, 565, 566, 592, 594, 620, 621, 622],\n",
       " [565, 566, 567, 593, 595, 621, 622, 623],\n",
       " [566, 567, 568, 594, 596, 622, 623, 624],\n",
       " [567, 568, 569, 595, 597, 623, 624, 625],\n",
       " [568, 569, 570, 596, 598, 624, 625, 626],\n",
       " [569, 570, 571, 597, 599, 625, 626, 627],\n",
       " [570, 571, 572, 598, 600, 626, 627, 628],\n",
       " [571, 572, 573, 599, 601, 627, 628, 629],\n",
       " [572, 573, 574, 600, 602, 628, 629, 630],\n",
       " [573, 574, 575, 601, 603, 629, 630, 631],\n",
       " [574, 575, 576, 602, 604, 630, 631, 632],\n",
       " [575, 576, 577, 603, 605, 631, 632, 633],\n",
       " [576, 577, 578, 604, 606, 632, 633, 634],\n",
       " [577, 578, 579, 605, 607, 633, 634, 635],\n",
       " [578, 579, 580, 606, 608, 634, 635, 636],\n",
       " [579, 580, 581, 607, 609, 635, 636, 637],\n",
       " [580, 581, 582, 608, 610, 636, 637, 638],\n",
       " [581, 582, 583, 609, 611, 637, 638, 639],\n",
       " [582, 583, 584, 610, 612, 638, 639, 640],\n",
       " [583, 584, 585, 611, 613, 639, 640, 641],\n",
       " [584, 585, 586, 612, 614, 640, 641, 642],\n",
       " [585, 586, 587, 613, 615, 641, 642, 643],\n",
       " [586, 587, 614, 642, 643],\n",
       " [588, 589, 617, 644, 645],\n",
       " [588, 589, 590, 616, 618, 644, 645, 646],\n",
       " [589, 590, 591, 617, 619, 645, 646, 647],\n",
       " [590, 591, 592, 618, 620, 646, 647, 648],\n",
       " [591, 592, 593, 619, 621, 647, 648, 649],\n",
       " [592, 593, 594, 620, 622, 648, 649, 650],\n",
       " [593, 594, 595, 621, 623, 649, 650, 651],\n",
       " [594, 595, 596, 622, 624, 650, 651, 652],\n",
       " [595, 596, 597, 623, 625, 651, 652, 653],\n",
       " [596, 597, 598, 624, 626, 652, 653, 654],\n",
       " [597, 598, 599, 625, 627, 653, 654, 655],\n",
       " [598, 599, 600, 626, 628, 654, 655, 656],\n",
       " [599, 600, 601, 627, 629, 655, 656, 657],\n",
       " [600, 601, 602, 628, 630, 656, 657, 658],\n",
       " [601, 602, 603, 629, 631, 657, 658, 659],\n",
       " [602, 603, 604, 630, 632, 658, 659, 660],\n",
       " [603, 604, 605, 631, 633, 659, 660, 661],\n",
       " [604, 605, 606, 632, 634, 660, 661, 662],\n",
       " [605, 606, 607, 633, 635, 661, 662, 663],\n",
       " [606, 607, 608, 634, 636, 662, 663, 664],\n",
       " [607, 608, 609, 635, 637, 663, 664, 665],\n",
       " [608, 609, 610, 636, 638, 664, 665, 666],\n",
       " [609, 610, 611, 637, 639, 665, 666, 667],\n",
       " [610, 611, 612, 638, 640, 666, 667, 668],\n",
       " [611, 612, 613, 639, 641, 667, 668, 669],\n",
       " [612, 613, 614, 640, 642, 668, 669, 670],\n",
       " [613, 614, 615, 641, 643, 669, 670, 671],\n",
       " [614, 615, 642, 670, 671],\n",
       " [616, 617, 645, 672, 673],\n",
       " [616, 617, 618, 644, 646, 672, 673, 674],\n",
       " [617, 618, 619, 645, 647, 673, 674, 675],\n",
       " [618, 619, 620, 646, 648, 674, 675, 676],\n",
       " [619, 620, 621, 647, 649, 675, 676, 677],\n",
       " [620, 621, 622, 648, 650, 676, 677, 678],\n",
       " [621, 622, 623, 649, 651, 677, 678, 679],\n",
       " [622, 623, 624, 650, 652, 678, 679, 680],\n",
       " [623, 624, 625, 651, 653, 679, 680, 681],\n",
       " [624, 625, 626, 652, 654, 680, 681, 682],\n",
       " [625, 626, 627, 653, 655, 681, 682, 683],\n",
       " [626, 627, 628, 654, 656, 682, 683, 684],\n",
       " [627, 628, 629, 655, 657, 683, 684, 685],\n",
       " [628, 629, 630, 656, 658, 684, 685, 686],\n",
       " [629, 630, 631, 657, 659, 685, 686, 687],\n",
       " [630, 631, 632, 658, 660, 686, 687, 688],\n",
       " [631, 632, 633, 659, 661, 687, 688, 689],\n",
       " [632, 633, 634, 660, 662, 688, 689, 690],\n",
       " [633, 634, 635, 661, 663, 689, 690, 691],\n",
       " [634, 635, 636, 662, 664, 690, 691, 692],\n",
       " [635, 636, 637, 663, 665, 691, 692, 693],\n",
       " [636, 637, 638, 664, 666, 692, 693, 694],\n",
       " [637, 638, 639, 665, 667, 693, 694, 695],\n",
       " [638, 639, 640, 666, 668, 694, 695, 696],\n",
       " [639, 640, 641, 667, 669, 695, 696, 697],\n",
       " [640, 641, 642, 668, 670, 696, 697, 698],\n",
       " [641, 642, 643, 669, 671, 697, 698, 699],\n",
       " [642, 643, 670, 698, 699],\n",
       " [644, 645, 673, 700, 701],\n",
       " [644, 645, 646, 672, 674, 700, 701, 702],\n",
       " [645, 646, 647, 673, 675, 701, 702, 703],\n",
       " [646, 647, 648, 674, 676, 702, 703, 704],\n",
       " [647, 648, 649, 675, 677, 703, 704, 705],\n",
       " [648, 649, 650, 676, 678, 704, 705, 706],\n",
       " [649, 650, 651, 677, 679, 705, 706, 707],\n",
       " [650, 651, 652, 678, 680, 706, 707, 708],\n",
       " [651, 652, 653, 679, 681, 707, 708, 709],\n",
       " [652, 653, 654, 680, 682, 708, 709, 710],\n",
       " [653, 654, 655, 681, 683, 709, 710, 711],\n",
       " [654, 655, 656, 682, 684, 710, 711, 712],\n",
       " [655, 656, 657, 683, 685, 711, 712, 713],\n",
       " [656, 657, 658, 684, 686, 712, 713, 714],\n",
       " [657, 658, 659, 685, 687, 713, 714, 715],\n",
       " [658, 659, 660, 686, 688, 714, 715, 716],\n",
       " [659, 660, 661, 687, 689, 715, 716, 717],\n",
       " [660, 661, 662, 688, 690, 716, 717, 718],\n",
       " [661, 662, 663, 689, 691, 717, 718, 719],\n",
       " [662, 663, 664, 690, 692, 718, 719, 720],\n",
       " [663, 664, 665, 691, 693, 719, 720, 721],\n",
       " [664, 665, 666, 692, 694, 720, 721, 722],\n",
       " [665, 666, 667, 693, 695, 721, 722, 723],\n",
       " [666, 667, 668, 694, 696, 722, 723, 724],\n",
       " [667, 668, 669, 695, 697, 723, 724, 725],\n",
       " [668, 669, 670, 696, 698, 724, 725, 726],\n",
       " [669, 670, 671, 697, 699, 725, 726, 727],\n",
       " [670, 671, 698, 726, 727],\n",
       " [672, 673, 701, 728, 729],\n",
       " [672, 673, 674, 700, 702, 728, 729, 730],\n",
       " [673, 674, 675, 701, 703, 729, 730, 731],\n",
       " [674, 675, 676, 702, 704, 730, 731, 732],\n",
       " [675, 676, 677, 703, 705, 731, 732, 733],\n",
       " [676, 677, 678, 704, 706, 732, 733, 734],\n",
       " [677, 678, 679, 705, 707, 733, 734, 735],\n",
       " [678, 679, 680, 706, 708, 734, 735, 736],\n",
       " [679, 680, 681, 707, 709, 735, 736, 737],\n",
       " [680, 681, 682, 708, 710, 736, 737, 738],\n",
       " [681, 682, 683, 709, 711, 737, 738, 739],\n",
       " [682, 683, 684, 710, 712, 738, 739, 740],\n",
       " [683, 684, 685, 711, 713, 739, 740, 741],\n",
       " [684, 685, 686, 712, 714, 740, 741, 742],\n",
       " [685, 686, 687, 713, 715, 741, 742, 743],\n",
       " [686, 687, 688, 714, 716, 742, 743, 744],\n",
       " [687, 688, 689, 715, 717, 743, 744, 745],\n",
       " [688, 689, 690, 716, 718, 744, 745, 746],\n",
       " [689, 690, 691, 717, 719, 745, 746, 747],\n",
       " [690, 691, 692, 718, 720, 746, 747, 748],\n",
       " [691, 692, 693, 719, 721, 747, 748, 749],\n",
       " [692, 693, 694, 720, 722, 748, 749, 750],\n",
       " [693, 694, 695, 721, 723, 749, 750, 751],\n",
       " [694, 695, 696, 722, 724, 750, 751, 752],\n",
       " [695, 696, 697, 723, 725, 751, 752, 753],\n",
       " [696, 697, 698, 724, 726, 752, 753, 754],\n",
       " [697, 698, 699, 725, 727, 753, 754, 755],\n",
       " [698, 699, 726, 754, 755],\n",
       " [700, 701, 729, 756, 757],\n",
       " [700, 701, 702, 728, 730, 756, 757, 758],\n",
       " [701, 702, 703, 729, 731, 757, 758, 759],\n",
       " [702, 703, 704, 730, 732, 758, 759, 760],\n",
       " [703, 704, 705, 731, 733, 759, 760, 761],\n",
       " [704, 705, 706, 732, 734, 760, 761, 762],\n",
       " [705, 706, 707, 733, 735, 761, 762, 763],\n",
       " [706, 707, 708, 734, 736, 762, 763, 764],\n",
       " [707, 708, 709, 735, 737, 763, 764, 765],\n",
       " [708, 709, 710, 736, 738, 764, 765, 766],\n",
       " [709, 710, 711, 737, 739, 765, 766, 767],\n",
       " [710, 711, 712, 738, 740, 766, 767, 768],\n",
       " [711, 712, 713, 739, 741, 767, 768, 769],\n",
       " [712, 713, 714, 740, 742, 768, 769, 770],\n",
       " [713, 714, 715, 741, 743, 769, 770, 771],\n",
       " [714, 715, 716, 742, 744, 770, 771, 772],\n",
       " [715, 716, 717, 743, 745, 771, 772, 773],\n",
       " [716, 717, 718, 744, 746, 772, 773, 774],\n",
       " [717, 718, 719, 745, 747, 773, 774, 775],\n",
       " [718, 719, 720, 746, 748, 774, 775, 776],\n",
       " [719, 720, 721, 747, 749, 775, 776, 777],\n",
       " [720, 721, 722, 748, 750, 776, 777, 778],\n",
       " [721, 722, 723, 749, 751, 777, 778, 779],\n",
       " [722, 723, 724, 750, 752, 778, 779, 780],\n",
       " [723, 724, 725, 751, 753, 779, 780, 781],\n",
       " [724, 725, 726, 752, 754, 780, 781, 782],\n",
       " [725, 726, 727, 753, 755, 781, 782, 783],\n",
       " [726, 727, 754, 782, 783],\n",
       " [728, 729, 757],\n",
       " [728, 729, 730, 756, 758],\n",
       " [729, 730, 731, 757, 759],\n",
       " [730, 731, 732, 758, 760],\n",
       " [731, 732, 733, 759, 761],\n",
       " [732, 733, 734, 760, 762],\n",
       " [733, 734, 735, 761, 763],\n",
       " [734, 735, 736, 762, 764],\n",
       " [735, 736, 737, 763, 765],\n",
       " [736, 737, 738, 764, 766],\n",
       " [737, 738, 739, 765, 767],\n",
       " [738, 739, 740, 766, 768],\n",
       " [739, 740, 741, 767, 769],\n",
       " [740, 741, 742, 768, 770],\n",
       " [741, 742, 743, 769, 771],\n",
       " [742, 743, 744, 770, 772],\n",
       " [743, 744, 745, 771, 773],\n",
       " [744, 745, 746, 772, 774],\n",
       " [745, 746, 747, 773, 775],\n",
       " [746, 747, 748, 774, 776],\n",
       " [747, 748, 749, 775, 777],\n",
       " [748, 749, 750, 776, 778],\n",
       " [749, 750, 751, 777, 779],\n",
       " [750, 751, 752, 778, 780],\n",
       " [751, 752, 753, 779, 781],\n",
       " [752, 753, 754, 780, 782],\n",
       " [753, 754, 755, 781, 783],\n",
       " [754, 755, 782]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-a3735f7c57bf>\", line 3, in <module>\n",
      "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 42, in __new__\n",
      "    return model.Var(name, dist, data, total_size)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 816, in Var\n",
      "    model=self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 1492, in __init__\n",
      "    transformed_name, transform.apply(distribution), total_size=total_size)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/transforms.py\", line 95, in apply\n",
      "    return TransformedDistribution.dist(dist, self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 52, in dist\n",
      "    dist.__init__(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/transforms.py\", line 125, in __init__\n",
      "    v = forward(FreeRV(name='v', distribution=dist))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/model.py\", line 1212, in __init__\n",
      "    self.logp_sum_unscaledt = distribution.logp_sum(self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/distribution.py\", line 119, in logp_sum\n",
      "    return tt.sum(self.logp(*args, **kwargs))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/continuous.py\", line 2355, in logp\n",
      "    beta > 0)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/dist_math.py\", line 51, in bound\n",
      "    return tt.switch(alltrue(conditions), logp, -np.inf)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/pymc3/distributions/dist_math.py\", line 57, in alltrue_elemwise\n",
      "    ret = ret * (1 * c)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/var.py\", line 233, in __rmul__\n",
      "    return theano.tensor.basic.mul(other, self)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/gof/op.py\", line 615, in __call__\n",
      "    node = self.make_node(*inputs, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/elemwise.py\", line 482, in make_node\n",
      "    DimShuffle, *inputs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/tensor/elemwise.py\", line 424, in get_output_info\n",
      "    for i in inputs])\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 1044, in make_node\n",
      "    for input in inputs])]\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 1052, in output_types\n",
      "    variables = self.output_types_preference(*types)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 838, in upcast_out\n",
      "    dtype = Scalar.upcast(*types)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 420, in upcast\n",
      "    return upcast(*[x.dtype for x in [self] + list(others)])\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/theano/scalar/basic.py\", line 78, in upcast\n",
      "    z = z + make_array(dt=dt)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/anaconda3/envs/ml/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    beta0  = pm.Normal('beta0', mu=0., tau=1e-2)\n",
    "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
    "    mu_phi = CAR2('mu_phi', w=wmat2, a=amat2, tau=tau, shape=N)\n",
    "    phi    = pm.Deterministic('phi', mu_phi-tt.mean(mu_phi)) # zero-center phi\n",
    "    \n",
    "    \n",
    "    mu = pm.Deterministic('mu', beta0 + phi)\n",
    "    Yi = pm.LogitNormal('Yi', mu=mu, observed=pad(O))\n",
    "    \n",
    "    trace = pm.sample(draws=1000, chains=2)\n",
    "    #trace = pm.sample(draws=2000, step=pm.Metropolis())\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check average proportion of pixels equal to 0 and 1 (or near 1, i.e. 253 or 254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pm.traceplot(trace, varnames=['beta0', 'tau_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.mean(trace.get_values('phi'), axis=0).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.std(trace.get_values('phi'), axis=0).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x_train[12].reshape(-1), bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying breast cancer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread('images/malignant_R_MLO.jpg')\n",
    "image = image[:, :, 0]\n",
    "image = np.max(image) - image\n",
    "x_dim, y_dim = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = []\n",
    "position_matrix = np.linspace(0, x_dim*y_dim - 1, num=x_dim*y_dim).astype(np.int64).reshape(x_dim, y_dim)\n",
    "count = 0\n",
    "\n",
    "for i, row in enumerate(position_matrix):\n",
    "    for j, col in enumerate(position_matrix[i]):\n",
    "        assert position_matrix[i][j] == col\n",
    "        \n",
    "        temp = []\n",
    "\n",
    "        # change these loops if we do not want to\n",
    "        # include diagonal elements in adj matrix\n",
    "        for delta_i in [-1, 0, 1]:\n",
    "            for delta_j in [-1, 0, 1]:\n",
    "                if ((i + delta_i) // x_dim == 0) and ((j + delta_j) // y_dim == 0):    \n",
    "                    temp.append(position_matrix[i + delta_i][j + delta_j])\n",
    "        \n",
    "\n",
    "        temp.remove(col)\n",
    "        temp.sort()\n",
    "        adj.append(temp)\n",
    "        \n",
    "weights = [list(np.ones_like(adj_elems).astype(np.int64)) for adj_elems in adj]\n",
    "\n",
    "# below is taken from the pymc3 CAR tutorial website\n",
    "maxwz = max([sum(w) for w in weights])\n",
    "N = len(weights)\n",
    "wmat = np.zeros((N, N))\n",
    "amat = np.zeros((N, N), dtype='int32')\n",
    "for i, a in enumerate(adj):\n",
    "    amat[i, a] = 1\n",
    "    wmat[i, a] = weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.zeros((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as a:\n",
    "    b = pm.Gamma('test', alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    beta0  = pm.Normal('beta0', mu=0., tau=1e-2)\n",
    "    tau    = pm.Gamma('tau_c', alpha=1.0, beta=1.0)\n",
    "    mu_phi = CAR2('mu_phi', w=wmat, a=amat, tau=tau, shape=N)\n",
    "    phi    = pm.Deterministic('phi', mu_phi-tt.mean(mu_phi)) # zero-center phi\n",
    "    \n",
    "    \n",
    "    mu = pm.Deterministic('mu', beta0 + phi)\n",
    "    Yi = pm.LogitNormal('Yi', mu=mu, observed=pad(O))\n",
    "    \n",
    "    trace = pm.sample(draws=1000, chains=2)\n",
    "    #trace = pm.sample(draws=2000, step=pm.Metropolis())\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name(name='phi_values', suffix='.npy', directory='results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact sparse CAR models in Stan\n",
    "\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.210407.svg)](https://doi.org/10.5281/zenodo.210407)\n",
    "\n",
    "\n",
    "\n",
    "Max Joseph  \n",
    "August 20, 2016  \n",
    "\n",
    "\n",
    "This document details sparse exact conditional autoregressive (CAR) models in Stan as an extension of previous work on approximate sparse CAR models in Stan. \n",
    "Sparse representations seem to give order of magnitude efficiency gains, scaling better for large spatial data sets. \n",
    "\n",
    "## CAR priors for spatial random effects\n",
    "\n",
    "Conditional autoregressive (CAR) models are popular as prior distributions for spatial random effects with areal spatial data. \n",
    "If we have a random quantity $\\phi = (\\phi_1, \\phi_2, ..., \\phi_n)'$ at $n$ areal locations, the CAR model is often expressed via full conditional distributions:\n",
    "\n",
    "$$\\phi_i \\mid \\phi_j, j \\neq i \\sim N(\\alpha \\sum_{j = 1}^n b_{ij} \\phi_j, \\tau_i^{-1})$$\n",
    "\n",
    "where $\\tau_i$ is a spatially varying precision parameter, and $b_{ii} = 0$. \n",
    "\n",
    "By Brook's Lemma, the joint distribution of $\\phi$ is then:\n",
    "\n",
    "$$\\phi \\sim N(0, [D_\\tau (I - \\alpha B)]^{-1}).$$\n",
    "\n",
    "If we assume the following:\n",
    "\n",
    "- $D_\\tau = \\tau D$\n",
    "- $D = diag(m_i)$: an $n \\times n$ diagonal matrix with $m_i$ = the number of neighbors for location $i$\n",
    "- $I$: an $n \\times n$ identity matrix\n",
    "- $\\alpha$: a parameter that controls spatial dependence ($\\alpha = 0$ implies spatial independence, and $\\alpha = 1$ collapses to an *intrisnic conditional autoregressive* (IAR) specification)\n",
    "- $B = D^{-1} W$: the scaled adjacency matrix\n",
    "- $W$: the adjacency matrix ($w_{ii} = 0, w_{ij} = 1$ if $i$ is a neighbor of $j$, and $w_{ij}=0$ otherwise)\n",
    "\n",
    "then the CAR prior specification simplifies to: \n",
    "\n",
    "$$\\phi \\sim N(0, [\\tau (D - \\alpha W)]^{-1}).$$\n",
    "\n",
    "The $\\alpha$ parameter ensures propriety of the joint distrbution of $\\phi$ as long as $| \\alpha | < 1$ (Gelfand & Vounatsou 2003).\n",
    "However, $\\alpha$ is often taken as 1, leading to the IAR specification which creates a singular precision matrix and an improper prior distribution.\n",
    "\n",
    "## A Poisson specification\n",
    "\n",
    "Suppose we have aggregated count data $y_1, y_2, ..., y_n$ at $n$ locations, and we expect that neighboring locations will have similar counts. \n",
    "With a Poisson likelihood: \n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\text{exp}(X_{i} \\beta + \\phi_i + \\log(\\text{offset}_i)))$$\n",
    "\n",
    "where $X_i$ is a design vector (the $i^{th}$ row from a design matrix), $\\beta$ is a vector of coefficients, $\\phi_i$ is a spatial adjustment, and $\\log(\\text{offset}_i)$ accounts for differences in expected values or exposures at the spatial units (popular choices include area for physical processes, or population size for disease applications). \n",
    "\n",
    "If we specify a proper CAR prior for $\\phi$, then we have that $\\phi \\sim \\text{N}(0, [\\tau (D - \\alpha W)]^{-1})$ where $\\tau (D - \\alpha W)$ is the precision matrix $\\Sigma^{-1}$.\n",
    "A complete Bayesian specification would include priors for the remaining parameters $\\alpha$, $\\tau$, and $\\beta$, such that our posterior distribution is: \n",
    "\n",
    "$$p(\\phi, \\beta, \\alpha, \\tau \\mid y) \\propto p(y \\mid \\beta, \\phi) p(\\phi \\mid \\alpha, \\tau) p(\\alpha) p(\\tau) p(\\beta)$$\n",
    "\n",
    "## Example: Scottish lip cancer data\n",
    "\n",
    "To demonstrate this approach we'll use the Scottish lip cancer data example (some documentation [here](https://cran.r-project.org/web/packages/CARBayesdata/CARBayesdata.pdf)). \n",
    "This data set includes observed lip cancer case counts at 56 spatial units in Scotland, with an expected number of cases to be used as an offset, and an area-specific continuous covariate that represents the proportion of the population employed in agriculture, fishing, or forestry.\n",
    "The model structure is identical to the Poisson model outlined above. \n",
    "\n",
    "\n",
    "```\n",
    "## Warning in gpclibPermit(): support for gpclib will be withdrawn from\n",
    "## maptools at the next major release\n",
    "```\n",
    "\n",
    "```\n",
    "## [1] TRUE\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/make-scotland-map-1.png)<!-- -->\n",
    "\n",
    "Let's start by loading packages and data, specifying the number of MCMC iterations and chains.\n",
    "\n",
    "\n",
    "```r\n",
    "library(ggmcmc)\n",
    "library(rstan)\n",
    "rstan_options(auto_write = TRUE)\n",
    "options(mc.cores = parallel::detectCores())\n",
    "source('data/scotland_lip_cancer.RData')\n",
    "\n",
    "# Define MCMC parameters \n",
    "niter <- 1E4   # definitely overkill, but good for comparison\n",
    "nchains <- 4\n",
    "```\n",
    "\n",
    "To fit the full model, we'll pull objects loaded with our Scotland lip cancer data. \n",
    "I'll use `model.matrix` to generate a design matrix, centering and scaling the continuous covariate `x` to reduce correlation between the intercept and slope estimates. \n",
    "\n",
    "\n",
    "```r\n",
    "W <- A # adjacency matrix\n",
    "scaled_x <- c(scale(x))\n",
    "X <- model.matrix(~scaled_x)\n",
    "  \n",
    "full_d <- list(n = nrow(X),         # number of observations\n",
    "               p = ncol(X),         # number of coefficients\n",
    "               X = X,               # design matrix\n",
    "               y = O,               # observed number of cases\n",
    "               log_offset = log(E), # log(expected) num. cases\n",
    "               W = W)               # adjacency matrix\n",
    "```\n",
    "\n",
    "#### Stan implementation: CAR with `multi_normal_prec`\n",
    "\n",
    "Our model statement mirrors the structure outlined above, with explicit normal and gamma priors on $\\beta$ and $\\tau$ respectively, and a $\\text{Uniform}(0, 1)$ prior for $\\alpha$. \n",
    "The prior on $\\phi$ is specified via the `multi_normal_prec` function, passing in $\\tau (D - \\alpha W)$ as the precision matrix.\n",
    "\n",
    "\n",
    "```\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W;\n",
    "}\n",
    "transformed data{\n",
    "  vector[n] zeros;\n",
    "  matrix<lower = 0>[n, n] D;\n",
    "  {\n",
    "    vector[n] W_rowsums;\n",
    "    for (i in 1:n) {\n",
    "      W_rowsums[i] = sum(W[i, ]);\n",
    "    }\n",
    "    D = diag_matrix(W_rowsums);\n",
    "  }\n",
    "  zeros = rep_vector(0, n);\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi;\n",
    "  real<lower = 0> tau;\n",
    "  real<lower = 0, upper = 1> alpha;\n",
    "}\n",
    "model {\n",
    "  phi ~ multi_normal_prec(zeros, tau * (D - alpha * W));\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "Fitting the model with `rstan`:\n",
    "\n",
    "\n",
    "```r\n",
    "full_fit <- stan('stan/car_prec.stan', data = full_d, \n",
    "                 iter = niter, chains = nchains, verbose = FALSE)\n",
    "print(full_fit, pars = c('beta', 'tau', 'alpha', 'lp__'))\n",
    "```\n",
    "\n",
    "```\n",
    "## Inference for Stan model: car_prec.\n",
    "## 4 chains, each with iter=10000; warmup=5000; thin=1; \n",
    "## post-warmup draws per chain=5000, total post-warmup draws=20000.\n",
    "## \n",
    "##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n",
    "## beta[1]   0.02    0.02 0.29  -0.52  -0.15   0.00   0.16   0.69   321 1.01\n",
    "## beta[2]   0.27    0.00 0.09   0.08   0.21   0.27   0.34   0.45  3981 1.00\n",
    "## tau       1.65    0.01 0.50   0.85   1.29   1.59   1.93   2.83  6218 1.00\n",
    "## alpha     0.93    0.00 0.06   0.77   0.91   0.95   0.98   1.00  3804 1.00\n",
    "## lp__    820.81    0.10 6.73 806.63 816.45 821.18 825.52 832.99  4485 1.00\n",
    "## \n",
    "## Samples were drawn using NUTS(diag_e) at Thu Feb  9 18:25:48 2017.\n",
    "## For each parameter, n_eff is a crude measure of effective sample size,\n",
    "## and Rhat is the potential scale reduction factor on split chains (at \n",
    "## convergence, Rhat=1).\n",
    "```\n",
    "\n",
    "```r\n",
    "# visualize results \n",
    "to_plot <- c('beta', 'tau', 'alpha', 'phi[1]', 'phi[2]', 'phi[3]', 'lp__')\n",
    "traceplot(full_fit, pars = to_plot)\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/fit-prec-model-1.png)<!-- -->\n",
    "\n",
    "### A more efficient sparse representation\n",
    "\n",
    "Although we could specify our multivariate normal prior for $\\phi$ directly in Stan via `multi_normal_prec`, as we did above, in this case we will accrue computational efficiency gains by manually specifying $p(\\phi \\mid \\tau, \\alpha)$ directly via the log probability accumulator. \n",
    "The log probability of $\\phi$ is: \n",
    "\n",
    "$$\\log(p(\\phi \\mid \\tau, \\alpha)) = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\text{det}( \\Sigma^{-1})) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "In Stan, we only need the log posterior up to an additive constant so we can drop the first term. \n",
    "Then, substituting  $\\tau (D - \\alpha W)$ for $\\Sigma^{-1}$:\n",
    "\n",
    "$$\\frac{1}{2} \\log(\\text{det}(\\tau (D - \\alpha W))) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$ = \\frac{1}{2} \\log(\\tau ^ n \\text{det}(D - \\alpha W)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$ = \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(\\text{det}(D - \\alpha W)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "There are two ways that we can accrue computational efficiency gains: \n",
    "\n",
    "1. Sparse representations of $\\Sigma^{-1}$ to expedite computation of $\\phi^T \\Sigma^{-1} \\phi$ (this work was done by Kyle foreman previously, e.g., https://groups.google.com/d/topic/stan-users/M7T7EIlyhoo/discussion). \n",
    "\n",
    "2. Efficient computation of the determinant. Jin, Carlin, and Banerjee (2005) show that:\n",
    "\n",
    "$$\\text{det}(D - \\alpha W) \\propto \\prod_{i = 1}^n (1 - \\alpha \\lambda_i)$$\n",
    "\n",
    "where $\\lambda_1, ..., \\lambda_n$ are the eigenvalues of $D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}$, which can be computed ahead of time and passed in as data. \n",
    "Because we only need the log posterior up to an additive constant, we can use this result which is proportional up to some multiplicative constant $c$: \n",
    "\n",
    "$$\\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(c \\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$= \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(c) +  \\frac{1}{2} \\log(\\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "Again dropping additive constants: \n",
    "\n",
    "$$\\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\log(\\prod_{i = 1}^n (1 - \\alpha \\lambda_i)) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "$$= \\frac{n}{2} \\log(\\tau) + \\frac{1}{2} \\sum_{i = 1}^n \\log(1 - \\alpha \\lambda_i) - \\frac{1}{2} \\phi^T \\Sigma^{-1} \\phi$$\n",
    "\n",
    "### Stan implementation: sparse CAR\n",
    "\n",
    "In the Stan model statement's `transformed data` block, we compute $\\lambda_1, ..., \\lambda_n$ (the eigenvalues of $D^{-\\frac{1}{2}} W D^{-\\frac{1}{2}}$), and generate a sparse representation for W (`Wsparse`), which is assumed to be symmetric, such that the adjacency relationships can be represented in a two column matrix where each row is an adjacency relationship between two sites. \n",
    "\n",
    "The Stan model statement for the sparse implementation never constructs the precision matrix, and does not call any of the `multi_normal*` functions. \n",
    "Instead, we use define a `sparse_car_lpdf()` function and use it in the model block. \n",
    "\n",
    "\n",
    "```\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper conditional autoregressive (CAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a CAR prior\n",
    "  * @param tau Precision parameter for the CAR prior (real)\n",
    "  * @param alpha Dependence (usually spatial) parameter for the CAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of CAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_car_lpdf(vector phi, real tau, real alpha, \n",
    "    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {\n",
    "      row_vector[n] phit_D; // phi' * D\n",
    "      row_vector[n] phit_W; // phi' * W\n",
    "      vector[n] ldet_terms;\n",
    "    \n",
    "      phit_D = (phi .* D_sparse)';\n",
    "      phit_W = rep_row_vector(0, n);\n",
    "      for (i in 1:W_n) {\n",
    "        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "      }\n",
    "    \n",
    "      for (i in 1:n) ldet_terms[i] = log1m(alpha * lambda[i]);\n",
    "      return 0.5 * (n * log(tau)\n",
    "                    + sum(ldet_terms)\n",
    "                    - tau * (phit_D * phi - alpha * (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W; // adjacency matrix\n",
    "  int W_n;                // number of adjacent region pairs\n",
    "}\n",
    "transformed data {\n",
    "  int W_sparse[W_n, 2];   // adjacency pairs\n",
    "  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)\n",
    "  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  { // generate sparse representation for W\n",
    "  int counter;\n",
    "  counter = 1;\n",
    "  // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1:(n - 1)) {\n",
    "      for (j in (i + 1):n) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1:n) D_sparse[i] = sum(W[i]);\n",
    "  {\n",
    "    vector[n] invsqrtD;  \n",
    "    for (i in 1:n) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi;\n",
    "  real<lower = 0> tau;\n",
    "  real<lower = 0, upper = 1> alpha;\n",
    "}\n",
    "model {\n",
    "  phi ~ sparse_car(tau, alpha, W_sparse, D_sparse, lambda, n, W_n);\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "Fitting the model:\n",
    "\n",
    "\n",
    "```r\n",
    "sp_d <- list(n = nrow(X),         # number of observations\n",
    "             p = ncol(X),         # number of coefficients\n",
    "             X = X,               # design matrix\n",
    "             y = O,               # observed number of cases\n",
    "             log_offset = log(E), # log(expected) num. cases\n",
    "             W_n = sum(W) / 2,    # number of neighbor pairs\n",
    "             W = W)               # adjacency matrix\n",
    "\n",
    "sp_fit <- stan('stan/car_sparse.stan', data = sp_d, \n",
    "               iter = niter, chains = nchains, verbose = FALSE)\n",
    "\n",
    "print(sp_fit, pars = c('beta', 'tau', 'alpha', 'lp__'))\n",
    "```\n",
    "\n",
    "```\n",
    "## Inference for Stan model: car_sparse.\n",
    "## 4 chains, each with iter=10000; warmup=5000; thin=1; \n",
    "## post-warmup draws per chain=5000, total post-warmup draws=20000.\n",
    "## \n",
    "##           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\n",
    "## beta[1]  -0.01    0.02 0.29  -0.63  -0.15   0.00   0.15   0.57   140 1.03\n",
    "## beta[2]   0.27    0.00 0.09   0.09   0.21   0.27   0.34   0.46  4449 1.00\n",
    "## tau       1.64    0.01 0.50   0.86   1.29   1.58   1.94   2.79  5808 1.00\n",
    "## alpha     0.93    0.00 0.06   0.76   0.91   0.95   0.97   0.99  3169 1.00\n",
    "## lp__    782.96    0.10 6.83 768.65 778.50 783.31 787.71 795.26  4418 1.00\n",
    "## \n",
    "## Samples were drawn using NUTS(diag_e) at Thu Feb  9 18:26:06 2017.\n",
    "## For each parameter, n_eff is a crude measure of effective sample size,\n",
    "## and Rhat is the potential scale reduction factor on split chains (at \n",
    "## convergence, Rhat=1).\n",
    "```\n",
    "\n",
    "```r\n",
    "traceplot(sp_fit, pars = to_plot)\n",
    "```\n",
    "\n",
    "![](README_files/figure-html/fit-sparse-model-1.png)<!-- -->\n",
    "\n",
    "### MCMC Efficiency comparison\n",
    " \n",
    "The main quantity of interest is the effective number of samples per unit time. \n",
    "Sparsity gives us an order of magnitude or so gains, mostly via reductions in run time. \n",
    "\n",
    "\n",
    "Model     Number of effective samples   Elapsed time (sec)   Effective samples / sec)\n",
    "-------  ----------------------------  -------------------  -------------------------\n",
    "full                         4485.084            488.56955                   9.180032\n",
    "sparse                       4418.415             38.52712                 114.683248\n",
    "\n",
    "### Posterior distribution comparison\n",
    "\n",
    "Let's compare the estimates to make sure that we get the same answer with both approaches. \n",
    "In this case, I've used more MCMC iterations than we would typically need in to get a better estimate of the tails of each marginal posterior distribution so that we can compare the 95% credible intervals among the two approaches. \n",
    "\n",
    "![](README_files/figure-html/compare-parameter-estimates-1.png)<!-- -->\n",
    "\n",
    "\n",
    "\n",
    "![](README_files/figure-html/unnamed-chunk-1-1.png)<!-- -->\n",
    "\n",
    "The two approaches give the same answers (more or less, with small differences arising due to MCMC sampling error). \n",
    "\n",
    "## Postscript: sparse IAR specification\n",
    "\n",
    "Although the IAR prior for $\\phi$ that results from $\\alpha = 1$ is improper, it remains popular (Besag, York, and Mollie, 1991). \n",
    "In practice, these models are typically fit with a sum to zero constraints: $\\sum_{i\\text{ in connected coponent}} \\phi_i = 0$ for each connected component of the graph. This allows us to interpret both the overall mean and the component-wise means.\n",
    "\n",
    "With $\\alpha$ fixed to one, we have: \n",
    "\n",
    "$$\\log(p(\\phi \\mid \\tau)) = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\text{det}^*(\\tau (D - W))) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "$$ = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\tau^{n-k} \\text{det}^*(D - W)) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "$$ = - \\frac{n}{2} \\log(2 \\pi) + \\frac{1}{2} \\log(\\tau^{n-k}) + \\frac{1}{2} \\log(\\text{det}^*(D - W)) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "Here $\\text{det}^*(A)$ is the generalized determinant of the square matrix $A$ defined as the product of its non-zero eigenvalues, and $k$ is the number of connected components in the graph. For the Scottish Lip Cancer data, there is only one connected component and $k=1$.  The reason that we need to use the generalized determinant is that the precision matrix is, by definition, singular in intrinsic models as the support of the Gaussian distribution is on a subspace with fewer than $n$ dimensions.  For the classical ICAR(1) model, we know that the directions correpsonding to the zero eigenvalues are exactly the vectors that are constant on each connected component of the graph and hence $k$ is the number of connected components.\n",
    "\n",
    "\n",
    "Dropping additive constants, the quantity to increment becomes: \n",
    "\n",
    "$$ \\frac{1}{2} \\log(\\tau^{n-k}) - \\frac{1}{2} \\phi^T \\tau (D - W) \\phi$$\n",
    "\n",
    "And the corresponding Stan syntax would be:\n",
    "\n",
    "\n",
    "```\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper intrinsic autoregressive (IAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a IAR prior\n",
    "  * @param tau Precision parameter for the IAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of IAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_iar_lpdf(vector phi, real tau,\n",
    "    int[,] W_sparse, vector D_sparse, vector lambda, int n, int W_n) {\n",
    "      row_vector[n] phit_D; // phi' * D\n",
    "      row_vector[n] phit_W; // phi' * W\n",
    "      vector[n] ldet_terms;\n",
    "    \n",
    "      phit_D = (phi .* D_sparse)';\n",
    "      phit_W = rep_row_vector(0, n);\n",
    "      for (i in 1:W_n) {\n",
    "        phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "        phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "      }\n",
    "    \n",
    "      return 0.5 * ((n-1) * log(tau)\n",
    "                    - tau * (phit_D * phi - (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> n;\n",
    "  int<lower = 1> p;\n",
    "  matrix[n, p] X;\n",
    "  int<lower = 0> y[n];\n",
    "  vector[n] log_offset;\n",
    "  matrix<lower = 0, upper = 1>[n, n] W; // adjacency matrix\n",
    "  int W_n;                // number of adjacent region pairs\n",
    "}\n",
    "transformed data {\n",
    "  int W_sparse[W_n, 2];   // adjacency pairs\n",
    "  vector[n] D_sparse;     // diagonal of D (number of neigbors for each site)\n",
    "  vector[n] lambda;       // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  { // generate sparse representation for W\n",
    "  int counter;\n",
    "  counter = 1;\n",
    "  // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1:(n - 1)) {\n",
    "      for (j in (i + 1):n) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1:n) D_sparse[i] = sum(W[i]);\n",
    "  {\n",
    "    vector[n] invsqrtD;  \n",
    "    for (i in 1:n) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  vector[p] beta;\n",
    "  vector[n] phi_unscaled;\n",
    "  real<lower = 0> tau;\n",
    "}\n",
    "transformed parameters {\n",
    "  vector[n] phi; // brute force centering\n",
    "  phi = phi_unscaled - mean(phi_unscaled);\n",
    "}\n",
    "model {\n",
    "  phi_unscaled ~ sparse_iar(tau, W_sparse, D_sparse, lambda, n, W_n);\n",
    "  beta ~ normal(0, 1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  y ~ poisson_log(X * beta + phi + log_offset);\n",
    "}\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "Besag, Julian, Jeremy York, and Annie Molli. \"Bayesian image restoration, with two applications in spatial statistics.\" Annals of the institute of statistical mathematics 43.1 (1991): 1-20.\n",
    "\n",
    "Gelfand, Alan E., and Penelope Vounatsou. \"Proper multivariate conditional autoregressive models for spatial data analysis.\" Biostatistics 4.1 (2003): 11-15.\n",
    "\n",
    "Jin, Xiaoping, Bradley P. Carlin, and Sudipto Banerjee. \"Generalized hierarchical multivariate CAR models for areal data.\" Biometrics 61.4 (2005): 950-961."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
